export const blogPosts = [
  {
    id: 1,
    title: "What is git? {#What-is-git}",
    excerpt: "--- Id: 1001 Title: Understanding Git and Github Author: Steve Tags: Git Github Topic: Git Abstract: Understanding the difference between Git and Github with examples and their main use cases or how they are helpful for the developers. HeaderImage: /BL-1001/header.jpg isPublished: true --- ## What is git? {#What-is-git} Git is...",
    date: "2025-01-01",
    readTime: "1 min read",
    category: "General",
    content: "---\nId: 1001\nTitle: Understanding Git and Github\nAuthor: Steve\nTags: Git Github\nTopic: Git\nAbstract: Understanding the  difference between Git and Github with examples and their main use cases or how they are helpful for the developers.\nHeaderImage: /BL-1001/header.jpg\nisPublished: true\n---\n\n## What is git? {#What-is-git}\n\nGit is a version control system which is maintained on your local system. Git gives us a record for ongoing programming versions. It can be used completely exclusive of any cloud-hosting service i.e. we don\u2019t even need internet to access git.\n\n## Version Control System {#Version-Control-System}\n\n![What is VCS](/BL-1001/vcs.png)\n\nIt is a program to keep track of the changes in projects, by tracking or logging the files changes over time. A version control system allows us to review, restore earlier versions and even branching and merging or code.\n\n## What Is GitHub? {#What-Is-GitHub}\n\nGitHub is a cloud-based hosting service that lets us manage Git repositories. It\u2019s an online database that allows you to keep track of and share your Git version control projects outside of your local computer.\nOther Git repository hosting services also exist: Eg:- GitLab, BitBucket, and SourceForge\n\nGitHub just takes things a little bit further than Git, offering more functionality and resources, as well as a place online to store and collaborate on projects. And because GitHub is cloud-based, an individual\u2019s Git repositories can be remotely accessed by any authorized person, from any computer, anywhere in the world.\n\nThrough GitHub, one can share code with others, giving them the power to make revisions or edits on your various Git branches. This makes it possible for entire teams to coordinate together on single projects in real-time. As changes are introduced, new branches are created, allowing the team to continue to revise the code without overwriting each other's work.\n\n## Conclusion {#Conclusion}\n\n![Diffrence between git and github](/BL-1001/gitVsGithub.webp)\n\n## Why Use a Version Control System? {#Why-Use-a-Version-Control-System}\n\n- Collaboration\n- Storing Versions\n- Restoring Previous Versions\n",
  },
  {
    id: 2,
    title: "Client Side Rendering {#Client-Side-Rendering}",
    excerpt: "--- Id: 1002 Title: Client Vs Server Side Rendering Author: Steve Tags: React Next Topic: React Abstract: Difference between different types of rendering techniques and advantage of Next.js over React.js. HeaderImage: /BL-1002/next-react.jpg isPublished: true --- ## Client Side Rendering {#Client-Side-Rendering} ![Client Side Rendering](/BL-1002/csr.png) When we talk about client-side rendering,it\u2019s about...",
    date: "2025-01-01",
    readTime: "3 min read",
    category: "General",
    content: "---\nId: 1002\nTitle: Client Vs Server Side Rendering\nAuthor: Steve\nTags: React Next\nTopic: React\nAbstract: Difference between different types of rendering techniques and advantage of Next.js over React.js.\nHeaderImage: /BL-1002/next-react.jpg\nisPublished: true\n---\n\n## Client Side Rendering {#Client-Side-Rendering}\n\n![Client Side Rendering](/BL-1002/csr.png)\n\nWhen we talk about client-side rendering,it\u2019s about rendering content in the browser using JavaScript.\n\nSo instead of getting all the content from the HTML document itself, a simple HTML document with a JavaScript file in initial loading itself is received, which renders the rest of the site using the browser.\n\nWith client-side rendering, the initial page load is naturally a bit slow. However, after that, every subsequent page load is very fast. In this approach, communication with server happens only for getting the run-time data. Moreover, there is no need to reload the entire UI after every call to the server. The client-side framework manages to update UI with changed data by re-rendering only that particular DOM element.\n\n## Server Side Rendering {#Server-Side-Rendering}\n\n![Server Side Rendering](/BL-1002/ssr.png)\n\nIn server-side rendering when a user makes a request to a webpage, the server prepares an HTML page by fetching user-specific data and sends it to the user\u2019s machine over the internet. Webpages are generated on your server for every request. This entire process of fetching data from the database, creating an HTML page and serve it to user is known as SSR.\n\n## Static Site Generation {#Static-Site-Generation}\n\nAt build time, your app will fetch all the data required and compile it down to static webpages. After a production build is created, every request is going to reuse that statically generated HTML file. This provides the best performance and can easily be cached on a CDN.\n\n## Problems with React.js {#Problems-with-React.js}\n\nReact uses Client Side Rendering. With React, nothing gets displayed until all of your JavaScript loads. Your HTML is nearly empty and React injects your content in your HTML with JavaScript.\nThis leads multiple problems:-\n\n- When the browser is loading the JavaScript, the screen is blank because `<div id=\"root\"></div>` does not show anything. Depending on the size of your JavaScript bundle, this could lead to your visitors staring at a white screen for a couple of seconds.\n\n- Most SEO crawlers do not have JavaScript enabled. DuckDuckGo, Google, Bing and any other search engine would not actually know what is on your website since it requires JavaScript to display the content. You will not be ranked at all on Search Engines.\n\n## Methods for Prerendering using Next.js {#Methods-for-Prerendering-using-Next.js}\n\nNext.js offers `(Server Side Rendering)SSR` and `(Static Site Generation)SSG` using `getStaticProps` and `getServerSideProps`.\n\n### getStaticProps {#getStaticProps}\n\ngetStaticProps is a server-side function that will only be called at build time. The build will then use the response from getStaticProps to generate a static webpage.\n\nSince stale data is a problem with static generated pages, there is an option you can set to revalidate your static page and rebuild it if data changes. revalidate: 60 will check your data every 60 seconds and rebuild the page if needed.\n\nExample Use:\n\n```\n// This function gets called at build time on server-side.\nexport async function getStaticProps() {\nconst res = await fetch('https://.../data');\nconst data = await res.json();\n\n// By returning { props: data }, the Dashboard component\n// will receive `data` as a prop at build time\nreturn {\nprops: {\ndata\n},\n// Check if data changes every 60 seconds.\n// Rebuild page if different\nrevalidate: 60\n};\n}\n\n// data will be populated at build time by getStaticProps()\nexport default function Dashboard({ data }) {\nreturn <div>{data}</div>;\n}\n```\n\n### getServerSideProps {#getServerSideProps}\n\ngetServerSideProps is similar to getStaticProps but is called every time the page loads instead of at build time. This ensures that all of your initial data is up to date on every load.\n\nSince this is called on every load, you do not need to revalidate like getStaticProps. This also leads to a slower load time since you are no longer serving a static file, but have to rebuild on every load.\n\nExample Use:\n\n```\n// This function gets called at build time on server-side.\nexport async function getServerSideProps() {\nconst res = await fetch('https://.../data');\nconst data = await res.json();\n\n// By returning { props: data }, the Dashboard component\n// will receive `data` as a prop at build time\nreturn {\nprops: {\ndata\n}\n};\n}\n\n// data will be populated at build time by getServerSideProps()\nexport default function Dashboard({ data }) {\nreturn <div>{data}</div>;\n}\n```\n\n## Benefits of using Next.js {#Benefits-of-using-Next.js}\n\n- Search engines can crawl the site for better SEO.\n- The initial page load is faster.\n  ![nextjs pros and cons](/BL-1002/nextjs-pros-and-cons.png)\n\n**References:**\n\n- <a href=\"https://nextjs.org/docs/getting-started\" target=\"_blank\">Next.js</a>\n",
  },
  {
    id: 3,
    title: "Why use react.js {#Why-use-react.js}",
    excerpt: "--- Id: 1003 Title: Core Concept You Need to Know About React Author: Steve Tags: React Interview Topic: React Abstract: Learning some of the fundamental concepts of React.js and top questions asked in react interview. HeaderImage: /BL-1003/reactjs.png isPublished: true --- ## Why use react.js {#Why-use-react.js} **_1.Reusable Components:_** React provides a...",
    date: "2025-01-01",
    readTime: "9 min read",
    category: "General",
    content: "---\nId: 1003\nTitle: Core Concept You Need to Know About React\nAuthor: Steve\nTags: React Interview\nTopic: React\nAbstract: Learning some of the fundamental concepts of React.js and top questions asked in react interview.\nHeaderImage: /BL-1003/reactjs.png\nisPublished: true\n---\n\n## Why use react.js {#Why-use-react.js}\n\n**_1.Reusable Components:_**\n\nReact provides a component based structure.Each tiny elements like button, checkbox, dropdown etc can be a component and the we create wrapper components composed of those smaller components.Each component decides how it should be rendered. Each component has its own internal logic.\n\n**_2.Fast render with Virtual DOM_**\n\nReact uses virtual DOM to render the view.virtual DOM is a virtual representation of the real DOM. Each time the data changes in a react app, a new virtual DOM gets created. Creating a virtual DOM is much faster than rendering the UI inside the browser. Therefore, with the use of virtual DOM, the efficiency of the app improves.\n\n**_3.SEO friendly_**\n\nReact allows developers to develop user interfaces that can be easily navigated in various search engines. It also allows server-side rendering, which boosts the SEO of an app.\n\n## What is JSX? {#What-is-JSX}\n\nJSX stands for JavaScript XML.\nIt allows us to write HTML inside JavaScript and place them in the DOM without using functions like `appendChild( )` or `createElement( )`.\n\n- Without using JSX, we would have to create an element by the following process:\n\n```\nconst text = React.createElement('p', {}, 'This is a text');\nconst container = React.createElement('div','{}',text );\nReactDOM.render(container,document.getElementById('app'));\n```\n\n- Using JSX, the above code can be simplified:\n\n````\nconst container = (\n\n <div>\n   <p>This is a text</p>\n </div>\n);\nReactDOM.render(container,document.getElementById('app'));```\n````\n\nAs one can see in the code above, we are directly using HTML inside JavaScript\\*\n\n## What is the virtual DOM? How does react use the virtual DOM to render the UI? {#What-is-the-virtual-DOM-How-does-react-use-the-virtual-DOM-to-render-the-UI}\n\n_DOM stands for 'Document Object Model'. In simple terms, it is a structured representation of the HTML elements that are present in a webpage or web-app. DOM represents the entire UI of your application_\n\nVirtual DOM is a concept where a virtual representation of the real DOM is kept inside the memory and is synced with the real DOM by a library such as ReactDOM.\n\n![Virtual Dom](/BL-1003/vdom.png)\n\n**Why was virtual DOM introduced?**\n\nDOM manipulation is an integral part of any web application, but DOM manipulation is quite slow when compared to other operations in JavaScript.\n\nThe efficiency of the application gets affected when several DOM manipulations are being done. Most JavaScript frameworks update the entire DOM even when a small part of the DOM changes.\n\nFor example, consider a list that is being rendered inside the DOM. If one of the items in the list changes, the entire list gets rendered again instead of just rendering the item that was changed/updated. This is called inefficient updating.\nTo address the problem of inefficient updating, the react team introduced the concept of virtual DOM.\n\n**How does it work?**\n\n![Virtual Dom](/BL-1003/real-virtual.png)\n\nFor every DOM object, there is a corresponding virtual DOM object(copy), which has the same properties.\n\nThe main difference between the real DOM object and the virtual DOM object is that any changes in the virtual DOM object will not reflect on the screen directly. Consider a virtual DOM object as a blueprint of the real DOM object.\nWhenever a JSX element gets rendered, every virtual DOM object gets updated.\n\nReact uses two virtual DOMs to render the user interface. One of them is used to store the current state of the objects and the other to store the previous state of the objects.\n\nWhenever the virtual DOM gets updated, react compares the two virtual DOMs and gets to know about which virtual DOM objects were updated.\n\nAfter knowing which objects were updated, react renders only those objects inside the real DOM instead of rendering the complete real DOM.This way, with the use of virtual DOM, react solves the problem of inefficient updating\n\n## Explain React state and props? {#Explain-React-state-and-props}\n\nA React component can access dynamic information in two ways: props and state.\n\n### React State {#React-State}\n\nEvery component in react has a built-in state object, which contains all the property values that belong to that component.\n\nIn other words, the state object controls the behaviour of a component. Any change in the property values of the state object leads to re-rendering of the component.\n\n_Note- State object is not available in functional components but, we can use React Hooks to add state to a functional component._\n\nHow to declare a state object?\n\n```\nExample:\nclass Car extends React.Component{\n constructor(props){\n   super(props);\n   this.state = {\n     brand: \"BMW\",\n     color: \"black\"\n   }\n }\n}\n```\n\nHow to use and update the state object?\n\n```\nclass Car extends React.Component {\n constructor(props) {\n   super(props);\n   this.state = {\n     brand: \"BMW\",\n     color: \"Black\"\n   };\n }\n\n changeColor() {\n   this.setState(prevState => {\n     return { color: \"Red\" };\n   });\n }\n\n render() {\n   return (\n     <div>\n       <button onClick={() => this.changeColor()}>Change Color</button>\n       <p>{this.state.color}</p>\n     </div>\n   );\n }\n}\n\n```\n\nAs one can see in the code above, we can use the state by calling this.state.propertyName and we can change the state object property using setState method.\n\n### React Props {#React-Props}\n\nEvery react component, accepts a single object argument called props (which stands for \u201cproperties\u201d).\n\nThese props can be passed to a component using HTML attributes.\n\nUsing props, we can pass data from one component to another.\n\n- While rendering a component, we can pass the props as a HTML attribute:\n\n```\n<Car brand=\"Mercedes\"/>\n```\n\nThe component receives the props:\n\n- In Class component:\n\n```\nclass Car extends React.Component {\n constructor(props) {\n   super(props);\n   this.state = {\n     brand: this.props.brand,\n     color: \"Black\"\n   };\n }\n}\n```\n\n- In Functional component:\n\n```\nfunction Car(props) {\n  render() {\n      return <h1>Name is {props.name}!</h1>;\n   }\n}\n```\n\n**Note- Props are read-only. They cannot be manipulated or changed inside a component**\n\n## Explain React Hooks? {#Explain-React-Hooks}\n\nHooks are functions that let us \u201chook into\u201d React state and lifecycle features from a `functional component.`\nReact Hooks cannot be used in class components.\n\nWhy were Hooks introduced in React?\nReact hooks were introduced in the 16.8 version of React.\nPreviously, functional components were called stateless components. Only class components were used for state management and lifecycle methods.\n\nThe need to change a functional component to a class component, whenever state management or lifecycle methods were to be used, led to the development of Hooks.\n\n**Example of a hook:**\n\n`useState hook:`\nIn functional components, useState hook lets us define state for a component:\n\n```\nfunction Person(props) {\n // We are declaring a state variable called name.\n // setName is a function to update/change the value of name\n let [name, setName] = useState('');\n}\n```\n\n## What are the different lifecycle methods in React? {#What-are-the-different-lifecycle-methods-in-React}\n\nEach component in react goes through three phases: `Mounting`, `Updating`, and `Unmounting`.\n\n- **Mounting** :- Birth of your component\n- **Update** :- Growth of your component\n- **Unmount** :- Death of your component\n\n![React Life Cycle Methods](/BL-1003/ReactLifeCycle.png)\n\n### Common React Lifecycle Methods {#Common-React-Lifecycle-Methods}\n\n- **constructor()**\n\nThis is used only if you have a class-based Component and it serves the purpose of initializing the state of a Component. In case of functional Components, the useState() hook is used to do the same.\n\n- **render()**\n\nThis is the method that is `responsible for inserting a Component into the DOM`.\n\nThe render() method is the most used lifecycle method. This is because render() is the only required method within a class component in React.\n\nIt happens during the `mounting` and `updating` of your component.\n\n```\nBelow is an example of a simple render() in React.\n\nclass Hello extends Component{\n   render(){\n      return <div>Hello {this.props.name}</div>\n   }\n}\n\n```\n\n**A render() can also return a null if there is nothing to render for that component.**\n\n- **componentDidMount()**\n\nThis is invoked `after a Component is inserted into the DOM for the first time`.\n\nWhen component has been mounted and ready, that\u2019s when the next React lifecycle method componentDidMount() comes in play.\n\ncomponentDidMount() is called as soon as the component is mounted and ready. This is a good place to initiate API calls, if you need to load data from a remote endpoint.\n\n`componentDidMount() allows the use of setState()`.\n\n- **componentDidUpdate()**\n\nThis is the method invoked after `re-rendering an updated Component`. This method can give you the information about a Component\u2019s previous state and previous props. This lifecycle method is invoked as soon as the updating happens. The most common use case for the componentDidUpdate() method is updating the DOM in response to prop or state changes.\n\nYou can call setState() in this lifecycle, but keep in mind that you will need to wrap it in a condition to check for state or prop changes from previous state. Incorrect usage of setState() can lead to an infinite loop.\n\n```\nTake a look at the example below that shows a typical usage example of this lifecycle method.\n\ncomponentDidUpdate(prevProps) {\n//Typical usage, don't forget to compare the props\nif (this.props.userName !== prevProps.userName) {\nthis.fetchData(this.props.userName);\n}\n}\n\n```\n\nNotice in the above example that we are comparing the current props to the previous props. This is to check if there has been a change in props from what it currently is. In this case, there won\u2019t be a need to make the API call if the props did not change.\n\n- **componentWillUnmount()**\n\nAs the name suggests this lifecycle method is called just `before the component is unmounted and destroyed`.\n\nYou cannot modify the component state in componentWillUnmount lifecycle.\n\nThis is where you can perform any cleanups that need to be done such as invalidating timers, canceling network requests, removing event listeners, and so on.\n\n```\ncomponentWillUnmount() {\nwindow.removeEventListener('resize', this.resizeListener)\n}\n```\n\n## What are keys in React? {#What-are-keys-in-React}\n\nA key is a special string attribute that needs to be included when using lists of elements.\n\nExample of a list using key:\n\n```\nconst ids = [1,2,3,4,5];\nconst listElements = ids.map((id)=>{\nreturn(\n <li key={id.toString()}>\n   {id}\n </li>\n )\n})\n```\n\n**Importance of keys:-**\n\n- Keys help react identify which elements were added, changed or removed.\n- Keys should be given to array elements for providing a unique identity for each element.\n- Without keys, React does not understand the order or uniqueness of each element.\n- With keys, React has an idea of which particular element was deleted,edited, and added.\n\n## React pure component {#React-pure-component}\n\nA React component is considered pure if it renders the same output for the same state and props. For class components like this, React provides the PureComponent base class. Class components that extend the `React.PureComponent` class are treated as `pure components`.\n\nPure components have some performance improvements and render optimizations since React implements the shouldComponentUpdate() method for them with a shallow comparison for props and state.\n\n**Features of React Pure Components**\n\n- Prevents re-rendering of Component if props or state is the same\n- Takes care of \u201cshouldComponentUpdate\u201d implicitly\n- State and Props are Shallow Compared\n- Pure Components are more performant in certain cases\n\n**React Components re-renders in the following scenarios:**\n\n- \u201csetState\u201d is called in Component\n- \u201cprops\u201d values are updated\n- this.forceUpdate() is called\n\n`In the case of Pure Components, the React components do not re-render blindly without considering the updated values of React \u201cprops\u201d and \u201cstate\u201d. If updated values are the same as previous values, render is not triggered.`\n",
  },
  {
    id: 4,
    title: "Global vs Local Scope {#Global-vs-Local-Scope}",
    excerpt: "--- Id: 1004 Title: Rough Author: Steve Tags: Interview Javascript Topic: Javascript Abstract: Learning some of the fundamental concepts of Javascript and top questions asked in javascript interview. HeaderImage: /BL-1001/header.jpg isPublished: false --- ## Global vs Local Scope {#Global-vs-Local-Scope} ## Async Defer ## Event bubbling capturing ## Event Throtting and...",
    date: "2025-01-01",
    readTime: "1 min read",
    category: "General",
    content: "---\nId: 1004\nTitle: Rough\nAuthor: Steve\nTags: Interview Javascript\nTopic: Javascript\nAbstract: Learning some of the fundamental concepts of Javascript and top questions asked in javascript interview.\nHeaderImage: /BL-1001/header.jpg\nisPublished: false\n---\n\n## Global vs Local Scope {#Global-vs-Local-Scope}\n\n## Async Defer\n\n## Event bubbling capturing\n\n## Event Throtting and debouncing\n",
  },
  {
    id: 5,
    title: "What is a Promise in JavaScript {#What-is-a-Promise-in-JavaScript}",
    excerpt: "--- Id: 1005 Title: Understanding Promises in Javasript Author: Steve Tags: Javascript Interview Topic: Javascript Abstract: Learning about Promises in Javascript HeaderImage: /BL-1005/header.png isPublished: true --- ## What is a Promise in JavaScript {#What-is-a-Promise-in-JavaScript} A Promise is a special JavaScript object. It produces a value after an asynchronous operation completes...",
    date: "2025-01-01",
    readTime: "2 min read",
    category: "General",
    content: "---\nId: 1005\nTitle: Understanding Promises in Javasript\nAuthor: Steve\nTags: Javascript Interview\nTopic: Javascript\nAbstract: Learning about Promises in Javascript\nHeaderImage: /BL-1005/header.png\nisPublished: true\n---\n\n## What is a Promise in JavaScript {#What-is-a-Promise-in-JavaScript}\n\nA Promise is a special JavaScript object. It produces a value after an asynchronous operation completes successfully, or an error if it does not complete successfully due to time out, network error, and so on.\n\n## Creating a JavaScript Promise {#Creating-a-JavaScript-Promise}\n\nWhen the task completes, you either fulfill your promise or fail to do so.\nPromise is a constructor function, so you need to use the `new keyword` to create one. It takes a function, as its argument, with two parameters - `resolve and reject`. These are methods used to determine the outcome of the promise.\n\n```\nCreating a Promise\n\nconst myPromise = new Promise((resolve, reject) => {\n});\n```\n\n## Promise States {#Promise-States}\n\nA promise has three states: `pending, fulfilled, and rejected.` The resolve and reject parameters given to the promise argument are used to do this. resolve is used when you want your promise to succeed, and reject is used when you want it to fail. These are methods that take an argument, as seen below.\n\n- **Pending: Initially when the executor function starts the execution.**\n- **Fulfilled: When the promise is resolved.**\n- **Rejected: When the promise is rejected.**\n\n![Promise States](/BL-1005/states.png)\n\n```\nconst myPromise = new Promise((resolve, reject) => {\n if(condition here) {\n   resolve(\"Promise was fulfilled\");\n } else {\n   reject(\"Promise was rejected\");\n }\n});\n```\n\n## Handling a Promise {#Handling-a-Promise}\n\n![Promise Handling](/BL-1005/handling.png)\n\n### Handling Promises With Then Method {#Handling-Promises-With-Then-Method}\n\nPromises are most useful when you have a process that takes an unknown amount of time in your code (i.e. something asynchronous), often a server request. When you make a server request it takes some amount of time, and after it completes you usually want to do something with the response from the server. This can be achieved by using the then method. The then method is executed immediately after your promise is fulfilled with resolve.\n\n```\nmyPromise.then(result => {\n\n});\n```\n\n### Handling a Rejected Promise with Catch Method {#Handling-a-Rejected-Promise-with-Catch-Method}\n\nCatch is the method used when your promise has been rejected. It is executed immediately after a promise's reject method is called.\n\n```\nmyPromise.catch(error => {\n\n});\n```\n\n## Async Await {#Async-Await}\n\nAsync/await are special syntax to work with promises in a more comfortable fashion.\n\n- **async makes a function return a Promise**\n- **await makes a function wait for a Promise**\n\n### Async {#Async}\n\n```\nExample\nasync function myFunction() {\n  return \"Hello\";\n}\n\nIs the same as:\nasync function myFunction() {\n  return Promise.resolve(\"Hello\");\n}\n```\n\n### Await {#Await}\n\nThe keyword await makes JavaScript wait until that promise settles and returns its result.\nThe `await keyword` can only be used inside an `async` function.\n\n```\nasync function f() {\n\n  let promise = new Promise((resolve, reject) => {\n    setTimeout(() => resolve(\"done!\"), 1000)\n  });\n\n  let result = await promise; // wait until the promise resolves (*)\n\n  alert(result); // \"done!\"\n}\n```\n\nThe function execution \"pauses\" at the line (\\*) and resumes when the promise settles, with result becoming its result.\n\n## Implement Promise {#Implement-Promise}\n\n```\nconst pr1 = new Promise((resolve, reject) => {\n  setTimeout(() => {\n    resolve(\"hello\");\n  }, 4000);\n});\n\nconst pr2 = new Promise((resolve, reject) => {\n  setTimeout(() => {\n    resolve(\"world\");\n  }, 0);\n});\n\nconst fun = async () => {\n  try {\n    const res = await Promise.all([pr1, pr2]);\n    console.log(res);\n  } catch (err) {\n    console.log(err);\n  }\n};\n\nfun();\n```\n\n**References:**\n\n- <a href=\"https://www.freecodecamp.org/news/javascript-promise-tutorial-how-to-resolve-or-reject-promises-in-js/\" target=\"_blank\">JavaScript Promise Tutorial</a>\n- <a href=\"https://www.w3schools.com/js/js_async.asp\" target=\"_blank\">Async/Await</a>\n",
  },
  {
    id: 6,
    title: "New Features in ES6 {#New-Features-in-ES6}",
    excerpt: "--- Id: 1006 Title: Javascript Concepts - Part1 Author: Steve Tags: Javascript Interview Topic: Javascript Abstract: Learning some of the fundamental concepts of Javascript and questions asked in javascript interview. HeaderImage: /BL-1006/header.png isPublished: true --- ## New Features in ES6 {#New-Features-in-ES6} Some of the new features of javascript introductes in...",
    date: "2025-01-01",
    readTime: "11 min read",
    category: "General",
    content: "---\nId: 1006\nTitle: Javascript Concepts - Part1\nAuthor: Steve\nTags: Javascript Interview\nTopic: Javascript\nAbstract: Learning some of the fundamental concepts of Javascript and questions asked in javascript interview.\nHeaderImage: /BL-1006/header.png\nisPublished: true\n---\n\n## New Features in ES6 {#New-Features-in-ES6}\n\nSome of the new features of javascript introductes in ES6 are:\n\n- The Let keyword\n- The Const keyword\n- Arrow Functions\n- Promises\n- Map Object\n\n```\n// Create a new Map\nconst fruits = new Map();\n\n// Add new Elements to the Map\nfruits.set(apples, 500);\nfruits.set(bananas, 300);\nfruits.set(oranges, 200);\n```\n\n- Set Object\n\n```\n// Create a Set\nconst letters = new Set();\n\n// Add some values to the Set\nletters.add(\"a\");\nletters.add(\"b\");\nletters.add(\"c\");\n```\n\n- Classes\n\n```\nclass ClassName {\n  constructor() { ... }\n}\n\n// Example\nclass Car {\n  constructor(name, year) {\n    this.name = name;\n    this.year = year;\n  }\n}\n```\n\n## Difference between Let and Var and Const {#Difference-between-Let-and-Var-and-Const}\n\n### Comparision of Declarations {#Comparision-of-Declarations}\n\nOne of the biggest problems with declaring variables with the var keyword is that you can easily overwrite variable declarations:\n\n```\nvar camper = \"James\";\nvar camper = \"David\";\nconsole.log(camper);\n```\n\nIn the code above, the camper variable is originally declared as James, and is then overridden to be David. The console then displays the string David.\n\nIf you replace var with let in the code above, it results in an error:\n\n```\nlet camper = \"James\";\nlet camper = \"David\";\nThe error can be seen in your browser console.(Uncaught SyntaxError: Identifier 'camper' has already been declared)\n```\n\n`So unlike var, when you use let, a variable with the same name can only be declared once.`\n\n### Comparision of scopes of the var and let Keywords {#Comparision-of-scopes-of-the-var-and-let-Keywords}\n\nWhen you declare a variable with the var keyword, it is declared globally.\nThe let keyword behaves similarly, but with some extra features. When you declare a variable with the let keyword inside a block, statement, or expression, its scope is limited to that block, statement, or expression.\n\n```\nvar numArray = [];\nfor (var i = 0; i < 3; i++) {\n  numArray.push(i);\n}\nconsole.log(numArray);\nconsole.log(i);\nHere the console will display the values [0, 1, 2] and 3.\n```\n\nWith the var keyword, i is declared globally. So when i++ is executed, it updates the global variable.\n\nThis behavior will cause problems if you were to create a function and store it for later use inside a for loop that uses the i variable. This is because the stored function will always refer to the value of the updated global i variable.\n\n```\nvar printNumTwo;\nfor (var i = 0; i < 3; i++) {\n  if (i === 2) {\n    printNumTwo = function() {\n      return i;\n    };\n  }\n}\nconsole.log(printNumTwo());\nHere the console will display the value 3.\n```\n\nAs you can see, printNumTwo() prints 3 and not 2. This is because the value assigned to i was updated and the printNumTwo() returns the global i and not the value i had when the function was created in the for loop.\n\nThe let keyword does not follow this behavior:\n\n```\nlet printNumTwo;\nfor (let i = 0; i < 3; i++) {\n    if (i === 2) {\n        printNumTwo = function() {\n            return i;\n        };\n    }\n}\nconsole.log(printNumTwo());\nconsole.log(i);\n```\n\nHere the console will display the value 2, and an error that i is not defined. i is not defined because it was not declared in the global scope. It is only declared within the for loop statement. printNumTwo() returned the correct value because three different i variables with unique values (0, 1, and 2) were created by the let keyword within the loop statement.\n\n### Const {#Const}\n\nWhile var and let can be declared without being initialized, const must be initialized during declaration.\n`Const cannot be initialized after declaritions`\n\nHowever, it is important to understand that objects (including arrays and functions) assigned to a variable using const are still mutable. Using the `const declaration only prevents reassignment of the variable identifier.`\n\n```\nconst s = [5, 6, 7];\ns = [1, 2, 3];\ns[2] = 45;\nconsole.log(s);\nThe console.log will display the value [5, 6, 45].\n\ns = [1, 2, 3] will result in an error.\n```\n\nAs you can see, you can mutate the object [5, 6, 7] itself and the variable s will still point to the altered array [5, 6, 45]. Like all arrays, the array elements in s are mutable, but because const was used, you cannot use the variable identifier s to point to a different array using the assignment operator.\n\n## Variable shadowing {#Variable-shadowing}\n\n```\nlet number = 10;\n\nfunction displayDouble() {\n  //new variable is defined with the same name as variable on line 1 - outer scope\n  let number = 3;\n\n  number *= 2;\n  console.log(number); //=> 6\n}\n\ndisplayDouble();\nconsole.log(number); //=> 10\n```\n\nIn this case, both variables on line 1 and 5 are defined with the same name \u2014 number.This has a significant result: the variable defined in the outer scope is \u2018shadowed\u2019 by the variable defined in the inner scope.\n\n## String in Javascript {#String-in-Javascript}\n\nIn JavaScript, String values are immutable, which means that they cannot be altered once created.\n\nFor example, the following code:\n\nlet myStr = \"Bob\";\nmyStr[0] = \"J\";\ncannot change the value of myStr to Job, because the contents of myStr cannot be altered.\n\n## Escape Sequences in Strings {#Escape-Sequences-in-Strings}\n\n```\nCode   Output\n\\' single quote\n\\\" double quote\n\\\\ backslash\n\\n newline\n\\r carriage return\n\\t tab\n\\b word boundary\n\\f form feed\n```\n\n## Difference between '==' and '===' in javascript {#Difference-between-==-and-===-in-javascript}\n\nStrict equality (===) is the counterpart to the equality operator (==). However, unlike the equality operator, which attempts to convert both values being compared to a common type, the strict equality operator does not perform a type conversion.\n\nIf the values being compared have different types, they are considered unequal, and the strict equality operator will return false.\n\n```\nExamples 1:\n\n3 === 3 //true\n3 === '3' // false\n1 == [1] //true\n1 === [1] //false\nnull == undefined //true\n```\n\n```\nExample 2:\u200c\nconst number = 1234\nconst stringNumber = '1234'\n\nconsole.log(number == stringNumber) //true\nconsole.log(number === stringNumber)  //false\n```\n\nThe value of number and stringNumber looks similar here. However, the type of number is Number and type of stringNumber is string. Even though the values are same, the type is not the same. Hence a == check returns true, but when checked for value and type, the value is false.\n\n```\nExample 3:\nconsole.log(0 == false) //true\nconsole.log(0 === false) //false\nReason: same value, different type. Type coercion\n```\n\nThis is an interesting case. The value of 0 when checked with false is same. It is so because 0 and false have the same value for JavaScript, but when checked for type and value, the value is false because 0 is a number and false is boolean.\n\n```\nExample 4:\nconst str = \"\"\n\nconsole.log(str == false) //true\nconsole.log(str === false) //false\n```\n\nThe value of empty string and false is same in JavaScript. Hence, == returns true. However, the type is different and hence === returns false.\n\n```\nExample 5:\nlet obj1 = { a: 1, b: 2 };\nlet obj2 = { a: 1, b: 2 };\nconsole.log(obj1 == obj2); //false\nconsole.log(obj1 === obj2); //false\n```\n\nThe important thing to understand here is that the variables, obj1 and obj2 (which could be an Object, Array or Function) each contain only a reference to a location in memory. Not the value of the object.\n\n## Explain how this works in JavaScript {#Explain-how-this-works-in-JavaScript}\n\nTo access a property of an object from within a method of the same object, you need to use the this keyword.\n\n```\nconst person = {\n    name: 'John',\n    age: 30,\n\n    // accessing name property by using this.name\n    greet: function() { console.log('The name is' + ' ' + this.name); }\n};\n\nperson.greet();\n```\n\nIn the above example, a person object is created. It contains properties (name and age) and a method greet.\nIn the method greet, while accessing a property of an object, this keyword is used.\nIn order to access the properties of an object, this keyword is used following by . and key.\n\nThis keyword refers to the `object where it is called.`\n\n### 1 this Inside Global Scope {#1-this-Inside-Global-Scope}\n\nWhen this is used alone, this refers to the global object (window object in browsers). For example,\n\n```\nlet a = this;\nconsole.log(a);  // Window {}\n\nthis.name = 'Sarah';\nconsole.log(window.name); // Sarah\n```\n\n### 2 this Inside Function {#2-this-Inside-Function}\n\nWhen this is used in a function, this refers to the global object (window object in browsers). For example,\n\n```\nfunction greet() {\n    // this inside function\n    // this refers to the global object\n    console.log(this);\n}\ngreet(); // Window {}\n```\n\n### 3 this Inside Constructor Function {#3-this-Inside-Constructor-Function}\n\nIn JavaScript, constructor functions are used to create objects. When a function is used as a constructor function, this refers to the object inside which it is used. For example,\n\n```\nfunction Person() {\n    this.name = 'Jack';\n    console.log(this);\n}\n\nlet person1 = new Person();\nconsole.log(person1.name);\n```\n\nHere, this refers to the person1 object. That's why, person1.name gives us Jack.\nWhen this is used with ES6 classes, it refers to the object inside which it is used.\n\nFor example,\n\n```\n// creating a class\nclass Person {\n  constructor(name) {\n    this.name = name;\n  }\n}\nThe class keyword is used to create a class. The properties are assigned in a constructor function.\n\n// creating a class\nclass Person {\n  constructor(name) {\n    this.name = name;\n  }\n}\n\n// creating an object\nconst person1 = new Person('John');\nconst person2 = new Person('Jack');\n\nconsole.log(person1.name); // John\nconsole.log(person2.name); // Jack\n```\n\n### 4 this Inside Object Method {#4-this-Inside-Object-Method}\n\nWhen this is used inside an object's method, this refers to the object it lies within. For example,\n\n```\nconst person = {\nname : 'Jack',\nage: 25,\n    // this inside method\n    // this refers to the object itself\n    greet() {\n        console.log(this);\n        console.log(this.name);\n    }\n\n}\n\nperson.greet();\nOutput\n{name: \"Jack\", age: 25, greet: \u0192}\nJack\n```\n\n### 5 this Inside Inner Function {#5-this-Inside-Inner-Function}\n\nWhen you access this inside an inner function (inside a method), this refers to the global object. For example,\n\n```\nconst person = {\nname : 'Jack',\nage: 25,\n\n    // this inside method\n    // this refers to the object itself\n    greet() {\n        console.log(this);        // {name: \"Jack\", age ...}\n        console.log(this.age);  // 25\n\n        // inner function\n        function innerFunc() {\n            // this refers to the global object\n            console.log(this);       // Window { ... }\n            console.log(this.age);    // undefined\n        }\n        innerFunc();\n    }\n}\nperson.greet();\n```\n\n### 6 this Inside Arrow Function {#6-this-Inside-Arrow-Function}\n\nInside the arrow function, this refers to the parent scope. For example,\n\n```\nconst greet = () => {\n    console.log(this);\n}\ngreet(); // Window {...}\n```\n\n`Arrow functions do not have their own this`. When you use this inside an arrow function, this refers to its parent scope object.\nFor example,\n\n```\nconst greet = {\n    name: 'Jack',\n\n    // method\n    sayHi () {\n        let hi = () => console.log(this.name);\n        hi();\n    }\n}\ngreet.sayHi(); // Jack\n```\n\n```\nconst person = {\n    name : 'Jack',\n    age: 25,\n\n    // this inside method\n    // this refers to the object itself\n    greet() {\n        console.log(this);\n        console.log(this.age);\n\n        // inner function\n        let innerFunc = () => {\n\n            // this refers to the global object\n            console.log(this);\n            console.log(this.age);\n        }\n        innerFunc();\n    }\n}\n\nperson.greet();\n\nOutput:\n{name: \"Jack\", age: 25, greet: \u0192}\n25\n{name: \"Jack\", age: 25, greet: \u0192}\n25\nHere, innerFunc() is defined using the arrow function. It takes this from its parent scope. Hence, this.age gives 25.\n```\n\n### 7 this Inside Function with Strict Mode {#7-this-Inside-Function-with-Strict-Mode}\n\nWhen this is used in a function with strict mode, this is undefined. For example,\n\n```\n'use strict';\nthis.name = 'Jack';\nfunction greet() {\n\n    // this refers to undefined\n    console.log(this);\n}\ngreet(); // undefined\n```\n\n## How is arrow functions different from normal functions in javascript {#How-is-arrow-functions-different-from-normal-functions-in-javascript}\n\n### this Keyword {#this-Keyword}\n\nInside a `regular function, this keyword refers to the function` where it is called.\n\nHowever, `this is not associated with arrow functions`. `Arrow function does not have its own this`. So whenever you call this, it refers to its parent scope.\n\n**Inside a regular function**\n\n```\nfunction Person() {\n    this.name = 'Jack',\n    this.age = 25,\n    this.sayName = function () {\n\n        // this is accessible\n        console.log(this.age);\n\n        function innerFunc() {\n            // this refers to the global object\n            console.log(this.age);\n            console.log(this);\n        }\n        innerFunc();\n    }\n}\n\nlet x = new Person();\nx.sayName();\n\nOutput:\n25\nundefined\nWindow {}\n```\n\nHere, this.age inside this.sayName() is accessible because this.sayName() is the method of an object.\n\nHowever, innerFunc() is a normal function and this.age is not accessible because this refers to the global object (Window object in the browser). Hence, this.age inside the innerFunc() function gives undefined.\n\n**Inside an arrow function**\n\n```\nfunction Person() {\n    this.name = 'Jack',\n    this.age = 25,\n    this.sayName = function () {\n\n        console.log(this.age);\n        let innerFunc = () => {\n            console.log(this.age);\n        }\n        innerFunc();\n    }\n}\n\nconst x = new Person();\nx.sayName();\n\nOutput\n25\n25\n```\n\nHere, the innerFunc() function is defined using the arrow function. And inside the arrow function, this refers to the parent's scope. Hence, this.age gives 25.\n\n### Arguments Binding {#Arguments-Binding}\n\n`Regular functions have arguments binding`. That's why when you pass arguments to a regular function, you can access them using the arguments keyword.\n\n```\nlet x = function () {\n    console.log(arguments);\n}\nx(4,6,7); // Arguments [4, 6, 7]\n```\n\n`Arrow functions do not have arguments binding`.\nWhen you try to access an argument using the arrow function, it will give an error. For example,\n\n```\nlet x = () => {\n    console.log(arguments);\n}\nx(4,6,7);\n// ReferenceError: Can't find variable: arguments\n```\n\nTo solve this issue, you can use the spread syntax.\n\n```\nlet x = (...n) => {\n  console.log(n);\n}\nx(4,6,7); // [4, 6, 7]\n```\n",
  },
  {
    id: 7,
    title: "Explain \"hoisting\" {#Explain-hoisting}",
    excerpt: "--- Id: 1007 Title: Javascript Concepts - Part2 Author: Steve Tags: Javascript Interview Topic: Javascript Abstract: Learning some of the fundamental concepts of Javascript and questions asked in javascript interview. HeaderImage: /BL-1006/header.png isPublished: true --- ## Explain \"hoisting\" {#Explain-hoisting} Hoisting is JavaScript's default behavior of moving all declarations to the...",
    date: "2025-01-01",
    readTime: "10 min read",
    category: "General",
    content: "---\nId: 1007\nTitle: Javascript Concepts - Part2\nAuthor: Steve\nTags: Javascript Interview\nTopic: Javascript\nAbstract: Learning some of the fundamental concepts of Javascript and questions asked in javascript interview.\nHeaderImage: /BL-1006/header.png\nisPublished: true\n---\n\n## Explain \"hoisting\" {#Explain-hoisting}\n\nHoisting is JavaScript's default behavior of moving all declarations to the top of the current scope.\nNote that the declaration is not actually moved - the JavaScript engine parses the declarations during compilation and becomes aware of declarations and their scopes.\n\n```\nconsole.log(foo); // undefined\nvar foo = 1;\nconsole.log(foo); // 1\n```\n\nFunction declarations have the body hoisted while the function expressions only has the variable declaration hoisted.\n\n```\n// Function Declaration\nconsole.log(foo); // [Function: foo]\nfoo(); // 'FOOOOO'\nfunction foo() {\n  console.log('FOOOOO');\n}\nconsole.log(foo); // [Function: foo]\n\n// Function Expression\nconsole.log(bar); // undefined\nbar(); // Uncaught TypeError: bar is not a function\nvar bar = function () {\n  console.log('BARRRR');\n};\nconsole.log(bar); // [Function: bar]\n```\n\nVariables defined with let and const are hoisted to the top of the block, but not initialized.Meaning that the block of code is aware of the variable, but it cannot be used until it has been declared.\n\n## What is a closure {#What-is-a-closure}\n\nJavaScript implements a scoping mechanism named lexical scoping (or static scoping). Lexical scoping means that the accessibility of variables is determined by the position of the variables inside the nested scopes.\n\nSimpler, the lexical scoping means that inside the inner scope you can access variables of outer scopes.\n\n**Closures are functions that have access to the outer (enclosing) function's variables scope even after the outer function has returned.**\n\n```\nfunction outerFunc() {\n  let outerVar = 'I am outside!';\n  function innerFunc() {\n    console.log(outerVar); // => logs \"I am outside!\"\n  }\n  return innerFunc;\n}\nfunction exec() {\n  const myInnerFunc = outerFunc();\n  myInnerFunc();\n}\nexec();\n```\n\nNow innerFunc() is executed outside of its lexical scope, but exactly in the scope of exec() function. And what's important:\ninnerFunc() still has access to outerVar from its lexical scope, even being executed outside of its lexical scope.\n\n## JavaScript Rest vs Spread Operator {#JavaScript-Rest-vs-Spread-Operator}\n\n### Rest Operator {#Rest-Operator}\n\n**The rest operator (...) is used to put the rest of some specific user-supplied values into a JavaScript array.**\n\nFor instance, consider this code that uses rest to enclose some values into an array:\n\n```\n// Use rest to enclose the rest of specific user-supplied values into an array:\nfunction myBio(firstName, lastName, ...otherInfo) {\n  return otherInfo;\n}\n\n// Invoke myBio function while passing five arguments to its parameters:\nmyBio(\"Oluwatobi\", \"Sofela\", \"CodeSweetly\", \"Web Developer\", \"Male\");\n\n// The invocation above will return:\n[\"CodeSweetly\", \"Web Developer\", \"Male\"]\n```\n\n### Spread Operator {#Spread-Operator}\n\n**The spread operator (...) helps you expand iterables into individual elements.**\nA spread operator is effective only when used within array literals, function calls, or initialized properties objects.\n\n### Example 1: How Spread Works in an Array Literal {#Example-1-How-Spread-Works-in-an-Array-Literal}\n\n```\nconst myName = [\"Sofela\", \"is\", \"my\"];\nconst aboutMe = [\"Oluwatobi\", ...myName, \"name.\"];\n\nconsole.log(aboutMe);\n\n// The invocation above will return:\n[ \"Oluwatobi\", \"Sofela\", \"is\", \"my\", \"name.\" ]\n```\n\nSuppose we did not use the spread syntax to duplicate myName\u2019s content. For instance, if we had written const aboutMe = [\"Oluwatobi\", myName, \"name.\"]. In such a case, the computer would have assigned a reference back to myName\n\n### Example 2: How to Use Spread to Convert a String into Individual Array Items {#Example-2-How-to-Use-Spread-to-Convert-a-String-into-Individual-Array-Items}\n\n```\nconst myName = \"Oluwatobi Sofela\";\n\nconsole.log([...myName]);\n\n// The invocation above will return:\n[ \"O\", \"l\", \"u\", \"w\", \"a\", \"t\", \"o\", \"b\", \"i\", \" \", \"S\", \"o\", \"f\", \"e\", \"l\", \"a\" ]\n```\n\n### Example 3: How the Spread Operator Works in a Function Call {#Example-3-How-the-Spread-Operator-Works-in-a-Function-Call}\n\n```\nconst numbers = [1, 3, 5, 7];\n\nfunction addNumbers(a, b, c, d) {\n  return a + b + c + d;\n}\n\nconsole.log(addNumbers(...numbers));\n\n// The invocation above will return:\n16\n```\n\nSuppose the numbers array had more than four items. In such a case, the computer will only use the first four items as addNumbers() argument and ignore the rest.\n\n```\nconst numbers = [1, 3, 5, 7, 10, 200, 90, 59];\n\nfunction addNumbers(a, b, c, d) {\n  return a + b + c + d;\n}\n\nconsole.log(addNumbers(...numbers));\n\n// The invocation above will return:\n16\n```\n\n### Example 4: How Spread Works in an Object Literal {#Example-4-How-Spread-Works-in-an-Object-Literal}\n\n```\nconst myNames = [\"Oluwatobi\", \"Sofela\"];\nconst bio = { ...myNames, runs: \"codesweetly.com\" };\n\nconsole.log(bio);\n\n// The invocation above will return:\n\n{ 0: \"Oluwatobi\", 1: \"Sofela\", runs: \"codesweetly.com\" }\n```\n\n- Spread operators can\u2019t expand object literal\u2019s values\n- Since a properties object is not an iterable object, you cannot use the spread operator to expand its values.\n- However, you can use the spread operator to clone properties from one object into another.\n\n```\nconst myName = { firstName: \"Oluwatobi\", lastName: \"Sofela\" };\nconst bio = { ...myName, website: \"codesweetly.com\" };\n\nconsole.log(bio);\n\n// The invocation above will return:\n{ firstName: \"Oluwatobi\", lastName: \"Sofela\", website: \"codesweetly.com\" };\n```\n\n## Pure vs Impure Functions in JavaScript {#Pure-vs-Impure-Functions-in-JavaScript}\n\n1. Pure Functions:\n\n- They must be predictable\n- They must have no side effects\n\nIdentical inputs will always return identical outputs, no matter how many times a pure function is called.\n\n2.  Impure Function:\n\n- Unpredictable\n- Has side-effects\n\nSide Effects can be:-\n\n- **Modifying a global variable**\n- **Modifying an argument**\n- **External dependency (APIs, outer variables)**\n- **DOM manipulation**\n- **Reading/writing files**\n\n```\n//IMPURE FUNCTION\nconst impureAddToArray = (arr1, num) => {\n  //altering arr1 in-place by pushing\n  arr1.push(num);\n  return arr1;\n};\n\n// PURE FUNCTION\n// Adding a value to an array via a pure function instead can be achieved using the spread operator, which makes a copy of the original array without mutating it.\n\nconst pureAddToArray = (arr1, num) => {\n  return [...arr1, num];\n};\n```\n\n```\n// IMPURE FUNCTION\nconst impureAddToObj = (obj, key, val) => {\n  obj[key] = val;\n  return obj;\n};\n\nBecause we're modifying the object in-place, the above approach is considered impure. Below is its pure counterpart, utilising the spread operator again.\n\n// PURE FUNCTION\nconst  pureAddToObj = (obj, key, val) => {\n  return { ...obj, [key]: val };\n}\n```\n\n## Higher Order Functions {#Higher-Order-Functions}\n\nA higher order function is a function that takes a function as an argument, or returns a function.\n\n- Some examples of higher order functions are .map() , .filter() and .reduce(). Both of them take a function as an argument.\n\n### Map Method {#Map-Method}\n\nUsing map method in javaScript creates an array by calling a specific function on each element present in the parent array.It returns a new array and elements of arrays are result of callback function.\n\n```\nSyntax:\narr.map(function(element, index, array){  }, this);\nThe this argument will be used inside the callback function. By default, its value is undefined .\n```\n\n```\nExample:\nlet arr = [2, 3, 5, 7]\n\narr.map(function(element, index, array){\n\tconsole.log(this) // 80\n}, 80);\n```\n\n### Filter Method {#Filter-Method}\n\nThe filter() method takes in a callback function and calls that function for every item it iterates over inside the target array. It entails filtering out one or more items (a subset) from a larger collection of items (a superset) based on some condition/preference.\n\n```\nSyntax:\narr.filter(function(element, index, array){  }, this);\nThe this argument will be used inside the callback function. By default, its value is undefined .\n```\n\n- Example: Filter items out of an array\n\n```\nlet people = [\n    {name: \"aaron\",age: 65},\n    {name: \"beth\",age: 2},\n    {name: \"cara\",age: 13},\n    {name: \"daniel\",age: 3},\n    {name: \"ella\",age: 25},\n    {name: \"fin\",age: 1},\n    {name: \"george\",age: 43},\n]\n\nlet toddlers = people.filter(person => person.age <= 3)\n\nconsole.log(toddlers)\n\n/*\n[{\n  age: 2,\n  name: \"beth\"\n}, {\n  age: 3,\n  name: \"daniel\"\n}, {\n  age: 1,\n  name: \"fin\"\n}]\n*/\n```\n\n- Example: How to access the context object with this\n\n```\nlet people = [\n    {name: \"aaron\", age: 65},\n    {name: \"beth\", age: 15},\n    {name: \"cara\", age: 13},\n    {name: \"daniel\", age: 3},\n    {name: \"ella\", age: 25},\n    {name: \"fin\", age: 16},\n    {name: \"george\", age: 18},\n]\n\nlet range = {\n  lower: 13,\n  upper: 16\n}\n\n\nlet teenagers = people.filter(function(person) {\n\treturn person.age >= this.lower && person.age <= this.upper;\n}, range)\n\nconsole.log(teenagers)\n\n/*\n[{\n  age: 15,\n  name: \"beth\"\n}, {\n  age: 13,\n  name: \"cara\"\n}, {\n  age: 16,\n  name: \"fin\"\n}]\n*/\n```\n\n### Reduce Method {#Reduce-Method}\n\n```\nSyntax\narray.reduce(function(total, currentValue, currentIndex, arr), initialValue)\n```\n\nArray.reduce takes two parameters.\n\n- The reducer\n- An initial value (optional)\n\n- The reducer is the function doing all the work. As reduce loops over your list, it feeds two parameters to your reducer.\n\n- An accumulator\n\nAccumulator is the eventual return value\nWhen you're looping through the users, how are you keeping track of their total age? You need some counter variable to hold it. That's the accumulator\n\n- The current value\n\nThe current value is just like when you use array[i] in a regular loop.\n\n- Example:\n\n```\nvar euros = [29.76, 41.85, 46.5];\n\nvar sum = euros.reduce( function(total, amount){\n  return total + amount\n}, 0);\n\nsum // 118.11\n```\n\n## Using call(), apply() and bind() {#Using-call-apply-and-bind}\n\nWe can have objects that have their own properties and methods.\nBut object1 cannot use the methods of object2 and vice versa.\n\n![CAB](/BL-1007/cab.png)\n\nWe can use call(), apply(), and bind() methods to tie a function into an object and call the function as if it belonged to that object.\n\n### Call() Method in JavaScript {#Call-Method-in-JavaScript}\n\nThe call() method invokes a function with a specified context.\n\n```\nvar obj = { firstName: \"a\", lastName:\"b\" };\n\nfunction fullName(){\n  return this.firstName + this.lastName;\n}\n```\n\nuse the call() method to tie the function add() to the object obj:\n\n```\nadd.call(obj, 3);\n```\n\n**Use Call() with Multiple Arguments**\n\n```\nfunction fullName(a, b){\n  return this.firstName + this.lastName + a + b;\n}\n\nconsole.log(add.call(obj, \"x\", \"y\"));\n```\n\n### Apply() Method in JavaScript {#Apply-Method-in-JavaScript}\n\nThe apply() method does the exact same as call(). The difference is that call() accepts an argument list, but apply() accepts an array of arguments.\n\n```\nvar obj = { firstName: \"a\", lastName:\"b\" };\n\nfunction fullName(){\n  return this.firstName + this.lastName;\n}\n\nconsole.log(add.apply(obj, [\"x\", \"y\"]));\n```\n\n### Bind() Method in JavaScript {#Bind-Method-in-JavaScript}\n\ncall() and apply() methods are executed immediately when called (and returned a value).\nBut instead of executing a function immediately, bind() returns a copy of a function that can be executed later on.\n\n```\nvar obj = { num: 2 };\n\nfunction add(a, b){\n  return this.num + a + b;\n}\n\nconst func = add.bind(obj, 3, 5);\nfunc(); // Returns 10\n```\n\n## Async and Defer {#Async-and-Defer}\n\n- Without using async and defer\n  The parsing is paused until the script is fetched, and executed. Once this is done, parsing resumes.\n\n![Async-Defer](/BL-1007/without-defer-async-head.png)\n\n- Page loading a script with async\n  The script is fetched asynchronously, and when it\u2019s ready the HTML parsing is paused to execute the script, then it\u2019s resumed.\n\n![Async](/BL-1007/with-async.png)\n\n- Page loading With defer\n  The script is fetched asynchronously, and it\u2019s executed only after the HTML parsing is done.\n\n![Defer](/BL-1007/with-defer.png)\n\n## Event Throtting and debouncing {#Event-Throtting-and-debouncing}\n\nDebouncing and Throttling techniques enhance the performance of your website, also prevent unnecessary API calls and load on the server.\n\nDebouncing and throttling techniques are used to limit the number of times a function can execute.\n\n### Throttling {#Throttling}\n\nThrottling is a technique in which, no matter how many times the user fires the event, the attached function will be executed only once in a given time interval.\n\n**Example**\n\n```\nlet timer;\n\nconst handleInput = (val, delay) => {\n  if (timer) {\n    return;\n  }\n\n  timer = setTimeout(() => {\n    console.log(val);\n    timer = undefined;\n  }, delay);\n};\n\ndocument.getElementById(\"search-box\").addEventListener(\"keypress\", (e) => {\n  handleInput(e.target.value, 1000);\n});\n```\n\n### Debouncing {#Debouncing}\n\nIn the debouncing technique, no matter how many times the user fires the event, the attached function will be executed only after the specified time once the user stops firing the event.\n\n**Example**\n\n```\nlet timer;\n\nconst handleInput = (val, delay) => {\n  clearTimeout(timer);\n\n  timer = setTimeout(() => {\n    console.log(val);\n  }, delay);\n};\n\ndocument.getElementById(\"search-box\").addEventListener(\"keypress\", (e) => {\n  handleInput(e.target.value, 1000);\n});\n```\n\n## Event Bubbling and Capturing {#Event-Bubbling-and-Capturing}\n\n![eventflow](/BL-1007/eventflow.svg)\n\n### Event Bubbling {#Event-Bubbling}\n\n**When an event happens on an element, it first runs the handlers on it, then on its parent, then all the way up on other ancestors.**\n\n```\nLet\u2019s say we have 3 nested elements FORM > DIV > P with a handler on each of them\n\n<form onclick=\"alert('form')\">FORM\n  <div onclick=\"alert('div')\">DIV\n    <p onclick=\"alert('p')\">P</p>\n  </div>\n</form>\n\nA click on the inner <p> first runs onclick:\n\nOn that <p>.\nThen on the outer <div>.\nThen on the outer <form>.\nAnd so on upwards till the document object.\n```\n\n![event-order-bubbling](/BL-1007/event-order-bubbling.svg)\n\n### Event Capturing {#Event-Capturing}\n\n**In event capturing, an event propagates from the outermost element to the target element.**\n\n![event-order-capture](/BL-1007/capture.svg)\n\n```\nClicking on the p element calls the click event handlers of all parent elements, starting from the outer and propagating inside to the target element p:\nhtml \u2192 body \u2192 article \u2192 div \u2192 p.\n```\n\n### Stopping bubbling {#Stopping-bubbling}\n\n```\nevent.stopPropagation()\n```\n\nevent.stopPropagation() stops the move upwards, but on the current element all other handlers will run.\n",
  },
  {
    id: 8,
    title: "Event Loop Visualisation/Basic Architecture {#Event-Loop-VisualisationBasic-Architecture}",
    excerpt: "--- Id: 1008 Title: Javascript Event Loop:Explained Author: Steve Tags: Javascript Interview Topic: Javascript Abstract: Understanding how javascript works under the hood, how it executes our asynchronous javascript code and event loop HeaderImage: /BL-1008/header.png isPublished: true --- Browser JavaScript execution flow, as well as in Node.js, is based on an...",
    date: "2025-01-01",
    readTime: "4 min read",
    category: "General",
    content: "---\nId: 1008\nTitle: Javascript Event Loop:Explained\nAuthor: Steve\nTags: Javascript Interview\nTopic: Javascript\nAbstract: Understanding how javascript works under the hood, how it executes our asynchronous javascript code and event loop\nHeaderImage: /BL-1008/header.png\nisPublished: true\n---\n\nBrowser JavaScript execution flow, as well as in Node.js, is based on an event loop. `The event loop concept is very simple. There\u2019s an endless loop, where the JavaScript engine waits for tasks, executes them, and then sleeps, waiting for more tasks.`\n\n`The event loop is the secret behind JavaScript\u2019s asynchronous programming.`\n\n## Event Loop Visualisation/Basic Architecture {#Event-Loop-VisualisationBasic-Architecture}\n\n![Event Loop](/BL-1008/eventloop.png)\n\n### Memory Heap {#Memory-Heap}\n\nThis is where all the memory allocation happens for your variables, that you have defined in your program.\n\n### Call Stack {#Call-Stack}\n\nThis represents the single thread provided for JavaScript code execution. This is where all your javascript code gets pushed and executed one by one as the interpreter reads your program, and gets popped out once the execution is done. It is responsible for keeping track of all the operations in line to be executed. Whenever a function is finished, it is popped from the stack.\n\n### Browser or Web APIs {#Browser-or-Web-APIs}\n\nThey are built into your web browser and are able to expose data from the browser and surrounding computer environment and do useful complex things with it. They are not part of the JavaScript language itself, rather they are built on top of the core JavaScript language, providing you with extra superpowers to use in your JavaScript code.\n\nFor example, the Geolocation API provides some simple JavaScript constructs for retrieving location data so you can say, plot your location on a Google Map. In the background, the browser is actually using some complex lower-level code (e.g. C++) to communicate with the device\u2019s GPS hardware (or whatever is available to determine position data), retrieve position data, and return it to the browser environment to use in your code. But again, this complexity is abstracted away from you by the API.\n\n**Common browser APIs**\n\n- APIs for manipulating documents\n- APIs that fetch data from the server\n- Audio and Video APIs\n- Client-side storage APIs\n\n### Event or Callback Queue {#Event-or-Callback-Queue}\n\nThis is where your asynchronous code gets pushed to, and waits for the execution.\n\n### Event Loop {#Event-Loop}\n\nIt has one simple job `to monitor the call stack and the callback queue.`\nIt keeps running continuously and checks the Main stack, if it has any frames to execute, if not then it checks Callback queue, if Callback queue has codes to execute then it pops the message from it to the Main Stack for the execution.\n\n### Job Queue or Microtask Queue {#Job-Queue-or-Microtask-Queue}\n\nApart from Callback Queue, browsers have introduced one more queue which is \u201cJob Queue\u201d, reserved only for new Promise() functionality. So when you use promises in your code, you add .then() method, which is a callback method. These `thenable` methods are added to Job Queue once the promise has returned/resolved, and then gets executed.\n\n## Execution of async function in javascript {#Execution-of-async-function-in-javascript}\n\nWhenever an async function is called, it is sent to a browser API. These are APIs built into the browser.\n\n`An example of this is the setTimeout method. When a setTimeout operation is processed, it is sent to the corresponding API which waits till the specified time to send this operation back in for processing.`\n\n`This operation is then send to the callback queue. Hence, we have a cyclic system for running async operations in JavaScript. The language itself is single-threaded, but the browser APIs act as separate threads.`\n\nThe `event loop` facilitates this process. It has one simple job `to monitor the call stack and the callback queue.` If the call stack is empty, the event loop will take the first event from the queue and will push it to the call stack, which effectively runs. If it is not, then the current function call is processed.\n\n```\nconst foo = () => console.log(\"First\");\nconst bar = () => setTimeout(() => console.log(\"Second\"), 500);\nconst baz = () => console.log(\"Third\");\n\nbar();\nfoo();\nbaz();\n\nOutput:\nFirst\nThird\nSecond\n```\n\n![Callback Queue](/BL-1008/callbackqueue.gif)\n\n## Why Job Queue or Microtask Queue {#Why-Job-Queue-or-Microtask-Queue}\n\nMicro-tasks has high priority in executing callbacks, if event loop tick comes to Micro-tasks, it will execute all the jobs in job queue first until it gets empty, then will move to callback queue.\n\n- The primary reason for prioritizing the micro-task queue is to improve the user experience.\n- Micro-tasks include mutation observer callbacks as well as promise callbacks.\n\n![micro-task Queue](/BL-1008/microtask.gif)\n\nExample:\n\n```\nconsole.log('Message no. 1: Sync');\n\nsetTimeout(function() {\n   console.log('Message no. 2: setTimeout');\n}, 0);\n\nvar promise = new Promise(function(resolve, reject) {\n   resolve();\n});\n\npromise.then(function(resolve) {\n   console.log('Message no. 3: 1st Promise');\n})\n.then(function(resolve) {\n   console.log('Message no. 4: 2nd Promise');\n});\n\nconsole.log('Message no. 5: Sync');\n\n// Message no. 1: Sync\n// Message no. 5: Sync\n// Message no. 3: 1st Promise\n// Message no. 4: 2nd Promise\n// Message no. 2: setTimeout\n```\n\nAll `thenable` callbacks of the promise are called first, then the setTimeout callback is called.\n\n**References:**\n\n- <a href=\"https://towardsdev.com/event-loop-in-javascript-672c07618dc9\" target=\"_blank\">Event Loop</a>\n",
  },
  {
    id: 9,
    title: "Prototype {#Prototype}",
    excerpt: "--- Id: 1009 Title: Prototype and Prototypical Inheritance Author: Steve Tags: Javascript Interview Topic: Javascript Abstract: Understanding And Using Prototype and Prototypical Inheritance in JavaScript HeaderImage: /BL-1009/header.jpg isPublished: true --- ## Prototype {#Prototype} ![Prototype](/BL-1009/object-prototype-empty.svg) **The prototype is an object that is associated with every functions and objects by default in...",
    date: "2025-01-01",
    readTime: "2 min read",
    category: "General",
    content: "---\nId: 1009\nTitle: Prototype and Prototypical Inheritance\nAuthor: Steve\nTags: Javascript Interview\nTopic: Javascript\nAbstract: Understanding And Using Prototype and Prototypical Inheritance in JavaScript\nHeaderImage: /BL-1009/header.jpg\nisPublished: true\n---\n\n## Prototype {#Prototype}\n\n![Prototype](/BL-1009/object-prototype-empty.svg)\n\n**The prototype is an object that is associated with every functions and objects by default in JavaScript.**\n\nWhenever we create a function , object or array javacript by default attaches a prototype object to it which contains some additional methods inside it.\n\n![Prototype](/BL-1009/proto.png)\nAll JavaScript objects inherit properties and methods from a prototype:\n\n- Date objects inherit from Date.prototype.\n- Array objects inherit from Array.prototype.\n- Player objects inherit from Player.prototype.\n\n- The Object.prototype is on top of the prototype inheritance chain. Date objects, Array objects, and Player objects all inherit from Object.prototype.\n\n## The Prototype Chain {#The-Prototype-Chain}\n\nPrototypal inheritance uses the concept of prototype chaining.\n\nEvery object created contains [[Prototype]], which points either to another object or null.\n\nExample:-\nAn object C with a [[Prototype]] property that points to object B. Object B\u2019s [[Prototype]] property points to prototype object A. This continues onward, forming a kind of chain called the prototype chain.\n\n## Prototypal Inheritance {#Prototypal-Inheritance}\n\n```\nlet animal = {\n  eats: true\n   walk() {\n    console.log(\"Animal walk\");\n  }\n};\n\nlet rabbit = {\n  jumps: true\n  __proto__ = animal;\n};\n\n\n// we can find both properties in rabbit now:\nconsole.log(rabbit.eats ); // true\n\nrabbit.walk(); // Animal walk\n\n```\n\n![Prototype](/BL-1009/rabbit-animal-object.svg)\n\n```\nconst obj = {\n  firstName: \"sds\",\n  lastName: \"bh\",\n  getFullName: function () {\n    return this.firstName + \" \" + this.lastName;\n  }\n};\n\nconst obj2 = {\n  firstName: \"ab\",\n  __proto__: obj\n};\n\nconsole.log(obj2.getFullName()); //ab bh\n```\n\n## Creating own prototype {#Creating-own-prototype}\n\nCreating Ployfill for bind method\n\n```\nconst obj = {\n  firstName: \"sds\",\n  lastName: \"bh\"\n};\n\nfunction getFullName(state) {\n  return this.firstName + \" \" + this.lastName + \" \" + state;\n}\n\nconst fName = getFullName.bind(obj, \"rnc\");\nconsole.log(fName()); //sds bh rnc\n\nFunction.prototype.myBind = function (...args) {\n  const func = this;\n  const params = args.slice(1);\n  return function () {\n    return func.apply(args[0], params);\n  };\n};\n\nconst fName2 = getFullName.myBind(obj, \"bsh\");\nconsole.log(fName2()); //sds bh bsh\n```\n\n## Creating Ployfill for Call, Apply and Bind method {#Creating-Ployfill-for-Call-Apply-and-Bind-method}\n\n```\nconst obj = {\n  firstName: \"sds\",\n  lastName: \"bh\"\n};\n\nfunction getFullName(state) {\n  return this.firstName + \" \" + this.lastName + \" \" + state;\n}\n\nFunction.prototype.myBind = function (obj, ...args) {\n  obj.func = this;\n  return () => {\n    return obj.func(...args);\n  };\n};\n\nFunction.prototype.myCall = function (obj, ...args) {\n  obj.func = this;\n  return obj.func(...args);\n};\n\nFunction.prototype.myApply = function (obj, args) {\n  obj.func = this;\n  return obj.func(...args);\n};\n\nconst fName2 = getFullName.myBind(obj, \"bsh\");\nconsole.log(fName2()); //sds bh bsh\n\nconsole.log(getFullName.myCall(obj, \"kkr\"));  //sds bh kkr\n\nconsole.log(getFullName.myApply(obj, [\"kkr\"]));  //sds bh kkr\n```\n",
  },
  {
    id: 10,
    title: "What is the React component lifecycle? {#What-is-the-React-component-lifecycle}",
    excerpt: "--- Id: 1010 Title: React Lifecycle Methods Author: Steve Tags: React Interview Topic: React Abstract: Lifecycle methods are series of events that happen throughout the birth, growth, and death of a React component. HeaderImage: /BL-1010/Header.jpg isPublished: true --- **Visual overview of topics covered in this tutorial** ![Lifecycle](/BL-1010/Lifecycle.png) ## What is...",
    date: "2025-01-01",
    readTime: "5 min read",
    category: "General",
    content: "---\nId: 1010\nTitle: React Lifecycle Methods\nAuthor: Steve\nTags: React Interview\nTopic: React\nAbstract: Lifecycle methods are series of events that happen throughout the birth, growth, and death of a React component.\nHeaderImage: /BL-1010/Header.jpg\nisPublished: true\n---\n\n**Visual overview of topics covered in this tutorial**\n\n![Lifecycle](/BL-1010/Lifecycle.png)\n\n## What is the React component lifecycle? {#What-is-the-React-component-lifecycle}\n\nIn React, components go through a lifecycle of events:\n\n- Mounting (adding nodes to the DOM)\n- Updating (altering existing nodes in the DOM)\n- Unmounting (removing nodes from the DOM)\n\n## Mounting lifecycle methods {#Mounting-lifecycle-methods}\n\nThe mounting phase refers to the phase during which a component is created and inserted to the DOM.\nThe following methods are called in order.\n\n- constructor()\n- static getDerivedStateFromProps()\n- render()\n- componentDidMount()\n\n### constructor() {#constructor}\n\nThe constructor() is the very first method that is invoked before the component is mounted to the DOM.\n\nThe constructor method is called before the component is mounted to the DOM. In most cases, you would initialize state and bind event handlers methods within the constructor method.\n\nExample of the constructor() React lifecycle method in action:\n\n```\nconst MyComponent extends React.Component {\n  constructor(props) {\n   super(props)\n    this.state = {\n       points: 0\n    }\n    this.handlePoints = this.handlePoints.bind(this)\n    }\n}\n```\n\n### static getDerivedStateFromProps() {#static-getDerivedStateFromProps}\n\nIts main function is to ensure that the state and props are in sync for when it\u2019s required.\n\nThe basic structure of the static getDerivedStateFromProps() looks like this:\n\n```\nstatic getDerivedStateFromProps(props, state) {\n  //do stuff here\n}\n```\n\n```\nYou can return an object to update the state of the component:\n\nstatic getDerivedStateFromProps(props, state) {\n    return {\n       points: 200 // update state with this\n    }\n}\n\nOr you can return null to make no updates:\n\nstatic getDerivedStateFromProps(props, state) {\n return null\n}\n```\n\n```\nclass App extends Component {\n  state = {\n    points: 10\n  }\n\n  // *******\n  //  NB: Not the recommended way to use this method. Just an example. Unconditionally overriding state here is generally considered a bad idea\n  // ********\n\n  static getDerivedStateFromProps(props, state) {\n    return {\n      points: 1000\n    }\n  }\n\n  render() {\n    return (\n      <div className=\"App\">\n        <header className=\"App-header\">\n          <img src={logo} className=\"App-logo\" alt=\"logo\" />\n          <p>\n            You've scored {this.state.points} points.\n          </p>\n        </header>\n      </div>\n    );\n  }\n}\n\nThe 1000 comes from updating state within the static getDerivedStateFromProps method.\n\n```\n\n### render() {#render}\n\nAs the name suggests it handles the rendering of your component to the UI\n\n```\nclass Hello extends Component{\n   render(){\n      return <div>Hello {this.props.name}</div>\n   }\n}\n```\n\nThe render() method returns JSX that is displayed in the UI. A render() can also return a null if there is nothing to render for that component.\n\nA render() method has to be pure with no side-effects.\nReact requires that your render() is pure. Pure functions are those that do not have any side-effects and will always return the same output when the same inputs are passed. This means that you can not `setState() within a render()`. `You cannot modify the component state within the render().`\n\n### componentDidMount {#componentDidMount}\n\nAfter render is called, `the component is mounted to the DOM and the componentDidMount method is invoked.`\n\nThis function is invoked immediately after the component is mounted to the DOM.\n\nYou would use the componentDidMount lifecycle method to grab a DOM node from the component tree immediately after it\u2019s mounted.\n\n**If you also want to make network requests as soon as the component is mounted to the DOM, this is a perfect place to do so.**\n\n```\ncomponentDidMount() {\n  this.fetchListOfTweets() // where fetchListOfTweets initiates a netowrk request to fetch a certain list of tweets.\n}\n```\n\n**You could also set up subscriptions such as timers**\n\n```\n// e.g requestAnimationFrame\ncomponentDidMount() {\n    window.requestAnimationFrame(this._updateCountdown);\n }\n```\n\n## Updating lifecycle methods {#Updating-lifecycle-methods}\n\nWhenever a change is made to the state or props of a React component, the component is rerendered. In simple terms, the component is updated. This is the updating phase of the React component lifecycle.\n\n- shouldComponentUpdate()\n- render()\n- getSnapshotBeforeUpdate()\n- componentDidUpdate()\n\n### shouldComponentUpdate() {#shouldComponentUpdate}\n\nIn most cases, you\u2019ll want a component to rerender when state or props changes. However, you do have control over this behavior.\n\nWithin this lifecycle method, you can return a boolean true or false and control whether the component gets rerendered (e.g., upon a change in state or props).\n\n```\nshouldComponentUpdate(nextProps, nextState)\n{\n return this.props.title !== nextProps.title ||\n  this.state.input !== nextState.input\n}\n```\n\n### render() {#render}\n\nAfter the shouldComponentUpdate method is called, render is called immediately afterward, depending on the returned value from shouldComponentUpdate, which defaults to true.\n\n### getSnapshotBeforeUpdate() {#getSnapshotBeforeUpdate}\n\nThe getSnapshotBeforeUpdatelifecycle method stores the previous values of the state after the DOM is updated. getSnapshotBeforeUpdate() is called right after the render method.\n\n```\ngetSnapshotBeforeUpdate(prevProps, prevState) {\n\n}\n```\n\nHere\u2019s the important thing: the value queried from the DOM in getSnapshotBeforeUpdate refers to the `value just before the DOM is updated, even though the render method was previously called`.\n\n`The getSnapshotBeforeUpdate React lifecycle method doesn\u2019t work on its own. It is meant to be used in conjunction with the componentDidUpdate lifecycle method`.\n\n### componentDidUpdate() {#componentDidUpdate}\n\nThe componentDidUpdate lifecycle method is invoked after the getSnapshotBeforeUpdate. As with the getSnapshotBeforeUpdate method it receives the previous props and state as arguments:\n\n```\ncomponentDidUpdate(prevProps, prevState) {\n\n}\n```\n\nHowever it also takes whatever value is returned from the getSnapshotBeforeUpdate lifecycle method is passed as the third argument to the componentDidUpdate method.\n\n```\ncomponentDidUpdate(prevProps, prevState, snapshot) {\n\n}\n```\n\n## Unmounting lifecycle method {#Unmounting-lifecycle-method}\n\nThe following method is invoked during the component unmounting phase\n\n- componentWillUnmount()\n\n### componentWillUnmount() {#componentWillUnmount}\n\nThe componentWillUnmount lifecycle method is invoked immediately before a component is unmounted and destroyed. This is the ideal place to perform any necessary cleanup such as clearing up timers, cancelling network requests, or cleaning up any subscriptions that were created in componentDidMount().\n\n```\n// e.g add event listener\ncomponentDidMount() {\n  el.addEventListener()\n}\n```\n\n```\n// e.g remove event listener\ncomponentWillUnmount() {\n  el.removeEventListener()\n}\n```\n\n**References:**\n\n- <a href=\"https://programmingwithmosh.com/javascript/react-lifecycle-methods\" target=\"_blank\">React Lifecycle Methods- A Deep Dive</a>\n- <a href=\"https://blog.logrocket.com/react-lifecycle-methods-tutorial-examples\" target=\"_blank\">React lifecycle methods: An approachable tutorial with examples</a>\n",
  },
  {
    id: 11,
    title: "What is Google Firebase? {#What-is-Google-Firebase}",
    excerpt: "--- Id: 1011 Title: Google Firebase Author: Steve Tags: Firebase Firestore Topic: Firebase Abstract: Understanding Firebase. This page offers brief overviews of several important concepts about Firebase. HeaderImage: /BL-1011/header.png isPublished: true --- ## What is Google Firebase? {#What-is-Google-Firebase} Google Firebase is a Google-backed application development software that enables developers to...",
    date: "2025-01-01",
    readTime: "8 min read",
    category: "General",
    content: "---\nId: 1011\nTitle: Google Firebase\nAuthor: Steve\nTags: Firebase Firestore\nTopic: Firebase\nAbstract: Understanding Firebase. This page offers brief overviews of several important concepts about Firebase.\nHeaderImage: /BL-1011/header.png\nisPublished: true\n---\n\n## What is Google Firebase? {#What-is-Google-Firebase}\n\nGoogle Firebase is a Google-backed application development software that enables developers to develop iOS, Android and Web apps. Firebase provides tools for tracking analytics, reporting and fixing app crashes, creating marketing and product experiment.\n\nFirebase is a powerful platform for your mobile and web application. Firebase can power your app\u2019s backend, including data storage, user authentication, static hosting, and more. With Firebase, you can easily build mobile and web apps that scale from one user to one million.\n\n## A Brief History {#A-Brief-History}\nBack in 2011, before Firebase was Firebase, it was a startup called Envolve. As Envolve, it provided developers with an API that enabled the integration of online chat functionality into their website.\n\nWhat\u2019s interesting is that people used Envolve to pass application data that was more than just chat messages. Developers were using Envolve to sync application data such as a game state in real time across their users.\n\nThis led the founders of Envolve, James Tamplin and Andrew Lee, to separate the chat system and the real-time architecture. In April 2012, Firebase was created as a separate company that provided Backend-as-a-Service with real-time functionality.\n\nAfter it was acquired by Google in 2014, Firebase rapidly evolved into the multifunctional behemoth of a mobile and web platform that it is today.\n\n## What services do Firebase provide? {#What-services-do-Firebase-provide}\n\nAnalytics \u2013 Google Analytics for Firebase offers free, unlimited reporting on as many as 500 separate events. Analytics presents data about user behavior in iOS and Android apps, enabling better decision-making about improving performance and app marketing. \n\nAuthentication \u2013 Firebase Authentication makes it easy for developers to build secure authentication systems and enhances the sign-in and onboarding experience for users. This feature offers a complete identity solution, supporting email and password accounts, phone auth, as well as Google, Facebook, GitHub, Twitter login and more.\n\nCloud messaging \u2013 Firebase Cloud Messaging (FCM) is a cross-platform messaging tool that lets companies reliably receive and deliver messages on iOS, Android and the web at no cost.\n\nRealtime database \u2013 the Firebase Realtime Database is a cloud-hosted NoSQL database that enables data to be stored and synced between users in real time. The data is synced across all clients in real time and is still available when an app goes offline.\n\nCrashlytics \u2013 Firebase Crashlytics is a real-time crash reporter that helps developers track, prioritize and fix stability issues that reduce the quality of their apps. With crashlytics, developers spend less time organizing and troubleshooting crashes and more time building features for their apps.\n\nPerformance \u2013 Firebase Performance Monitoring service gives developers insight into the performance characteristics of their iOS and Android apps to help them determine where and when the performance of their apps can be improved.\n\nTest lab \u2013 Firebase Test Lab is a cloud-based app-testing infrastructure. With one operation, developers can test their iOS or Android apps across a variety of devices and device configurations. They can see the results, including videos, screenshots and logs, in the Firebase console.\n\nFirebase helps mobile app teams succeed. With Firebase you can:\n\n1. Build better apps\n2. Improve app quality\n3. Grow your business\n\n### Build better apps {#Build-better-apps}\nFirebase lets you build more powerful, secure and scalable apps, using world-class infrastructure using:\n\nCloud Firestore: is a flexible, scalable database for mobile, web, and server development from Firebase and Google Cloud Platform. It is a NoSQL document database that lets you easily store, sync, and query data for your mobile and web apps \u2014 at a global scale.It's supporting for Android, iOS and Web Platform.\n\nML Kit: is a mobile SDK that brings Google\u2019s machine learning expertise to Android and iOS apps in a powerful yet easy-to-use package. Whether you\u2019re new or experienced in machine learning, you can implement the functionality you need in just a few lines of code. There\u2019s no need to have deep knowledge of neural networks or model optimization to get started. On the other hand, if you are an experienced ML developer, ML Kit provides convenient APIs that help you use your custom TensorFlow Lite models in your mobile apps. It\u2019s supporting for Android and iOS Platform.\n\nCloud Functions: for Firebase lets you automatically run backend code in response to events triggered by Firebase features and HTTPS requests. Your code is stored in Google\u2019s cloud and runs in a managed environment. There\u2019s no need to manage and scale your own servers. It\u2019s supporting for Android, iOS, C++, Unity and Web Platform.\n\nAuthentication: provides backend services, easy-to-use SDKs, and ready-made UI libraries to authenticate users to your app. It supports authentication using passwords, phone numbers, popular federated identity providers like Google, Facebook and Twitter, and more. It\u2019s supporting for Android, iOS and Web Platform.\n\nHosting: is production-grade web content hosting for developers. With a single command, you can quickly deploy web apps and serve both static and dynamic content to a global CDN (content delivery network). You can also pair Firebase Hosting with Cloud Functions to build and host microservices on Firebase. It\u2019s supporting only Web Platform.\n\nCloud Storages: is for object storage service built for Google scale. The Firebase SDKs for Cloud Storage add Google security to file uploads and downloads for your Firebase apps, regardless of network quality. You can use our SDKs to store images, audio, video, or other user-generated content. On the server, you can use Google Cloud Storage, to access the same files. It\u2019s supporting for Android, iOS, C++, Unity and Web Platform.\n\nReal-time Database: is a cloud-hosted NoSQL database that lets you store and sync between your users in real-time. The Real-time Database is really just one big JSON object that the developers can manage in real-time. It\u2019s supporting for Android, iOS, C++, Unity and Web Platform.\n\n\n### Improve app quality {#Improve-app-quality}\nFirebase gives you insights into app performance and stability, so you can channel your resources effectively using:\n\nCrashlytics: is a lightweight, real-time crash reporter that helps you track, prioritize, and fix stability issues that erode your app quality. Crashlytics saves you troubleshooting time by intelligently grouping crashes and highlighting the circumstances that lead up to them. It\u2019s supporting for Android and iOS Platform.\n\nPerformance Monitoring: is a service that helps you to gain insight into the performance characteristics of your iOS and Android apps. You use the Performance Monitoring SDK to collect performance data from your app, and then review and analyze that data in the Firebase console. Performance Monitoring helps you to understand where and when the performance of your app can be improved so that you can use that information to fix performance issues. It\u2019s supporting for Android and iOS Platform.\n\nTest Labs: is a cloud-based app-testing infrastructure. It provides a large number of mobile test devices to help you test your apps. It\u2019s supporting for Android and iOS Platform.\n\n\n### Grow your business {#Grow-your-business}\nFirebase helps you grow to millions of users, simplifying user engagement and retention using:\n\nIn-App Messaging: helps you engage users who are actively using your app by sending them targeted and contextual messages that nudge them to complete key in-app actions \u2014 like beating a game level, buying an item, or subscribing to content. It\u2019s supporting for Android and iOS Platform.\n\nGoogle Analytics: for Firebase is a free app measurement solution that provides insight on app usage and user engagement. At the heart of Firebase is Google Analytics for Firebase, a free and unlimited analytics solution. Analytics integrates across Firebase features and provides you with unlimited reporting for up to 500 distinct events that you can define using the Firebase SDK. Analytics reports help you understand clearly how your users behave, which enables you to make informed decisions regarding app marketing and performance optimizations. It\u2019s supporting for Android, iOS, C++ and Unity Platform.\n\nPredictions: applies machine learning to your analytics data to create dynamic user segments based on the predicted behavior of users in your app. These predictions are automatically available for use with Firebase Remote Config, the Notifications composer, Firebase In-App Messaging, and A/B Testing. You can also link your app\u2019s Predictions data to BigQuery so you can get daily exports that you can further analyze or push to third party tools. It\u2019s supporting for Android, iOS, C++ and Unity Platform.\n\nA/B Testing: helps you optimize your app experience by making it easy to run, analyze, and scale product and marketing experiments. It gives you the power to test changes to your app\u2019s UI, features, or engagement campaigns to see if they actually move the needle on your key metrics (like revenue and retention) before you roll them out widely. It\u2019s supporting for Android, iOS, C++ and Unity Platform.\n\nCloud Messaging(FCM): provides a reliable and battery-efficient connection between your server and devices that allows you to deliver and receive messages and notifications on iOS, Android, and the web at no cost.\n\nRemote Config: essentially allows us to publish updates to our users immediately. Whether we wish to change the color scheme for a screen, the layout for a particular section in our app or show promotional/seasonal options \u2014 this is completely doable using the server side parameters without the need to publish a new version. It\u2019s supporting for Android, iOS, C++ and Unity Platform.\n\nDynamic Links: With Dynamic Links, your users get the best available experience for the platform they open your link on. If a user opens a Dynamic Link on iOS or Android, they can be taken directly to the linked content in your native app. If a user opens the same Dynamic Link in a desktop browser, they can be taken to the equivalent content on your website. It\u2019s supporting for Android, iOS, Web, C++ and Unity Platform.\n\nApp Indexing: gets your app into Google Search. If users have your app installed, they can launch your app and go directly to the content they\u2019re searching for. App Indexing reengages your app users by helping them find both public and personal content right on their device, even offering query autocompletion to help them more quickly find what they need. If users don\u2019t yet have your app, relevant queries trigger an install card for your app in Search results. It\u2019s supporting for Android and iOS Platform.\n\n## Understand Firebase projects {#Understand-Firebase-projects}\n\nA Firebase project is the top-level entity for Firebase. In a project, you can register your Apple, Android, or web apps. After you register your apps with Firebase, you can add the Firebase SDKs for any number of Firebase products, like Analytics, Cloud Firestore, Performance Monitoring, or Remote Config.\n\n",
  },
  {
    id: 12,
    title: "Get started with Tailwind CSS {#Get-started-with-Tailwind-CSS}",
    excerpt: "--- Id: 1012 Title: Understanding Tailwind CSS Author: Steve Tags: Tailwind Topic: Tailwind CSS Abstract: All of the components in Tailwind UI are designed for the latest version of Tailwind CSS, which is currently Tailwind CSS v3.3. To make sure that you are on the latest version of Tailwind, update...",
    date: "2025-01-01",
    readTime: "1 min read",
    category: "General",
    content: "---\nId: 1012\nTitle: Understanding Tailwind CSS\nAuthor: Steve\nTags: Tailwind\nTopic: Tailwind CSS\nAbstract: All of the components in Tailwind UI are designed for the latest version of Tailwind CSS, which is currently Tailwind CSS v3.3. To make sure that you are on the latest version of Tailwind, update via npm\nHeaderImage: /BL-1012/header.png\nisPublished: true\n---\n\n## Get started with Tailwind CSS {#Get-started-with-Tailwind-CSS}\n\nTailwind CSS works by scanning all of your HTML files, JavaScript components, and any other templates for class names, generating the corresponding styles and then writing them to a static CSS file.\nIt's fast, flexible, and reliable \u2014 with zero-runtime.\n\n###  Installation\n\nHey",
  },
  {
    id: 13,
    title: "Introduction {#Introduction}",
    excerpt: "--- Id: 1013 Title: Introduction to Amazon EC2 Author: Steve Tags: AWS Topic: Amazon EC2 Abstract: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. HeaderImage: /BL-1013/header.webp isPublished: true ---...",
    date: "2025-01-01",
    readTime: "9 min read",
    category: "General",
    content: "---\nId: 1013\nTitle: Introduction to Amazon EC2\nAuthor: Steve\nTags: AWS\nTopic: Amazon EC2\nAbstract: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers.\nHeaderImage: /BL-1013/header.webp\nisPublished: true\n---\n\n## Introduction {#Introduction}\nAmazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers.\n\nAmazon EC2\u2019s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon\u2019s proven computing environment. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change.\n\nAmazon EC2 changes the economics of computing by allowing you to pay only for capacity that you actually use. Amazon EC2 provides developers the tools to build failure-resilient applications and isolate themselves from common failure scenarios.\n\n### Topics covered {#Topics-covered}\n\n1. Launch a web server with termination protection enabled\n2. Monitor Your EC2 instance\n\n3. Modify the security group that your web server is using to allow HTTP access\n\n4. Resize your Amazon EC2 instance to scale\n\n5. Test termination protection\n\n6. Terminate your EC2 instance\n\n### 1. Launching your EC2 instance {#1.-Launching-your-EC2-instance}\nIn the AWS Management Console on the Services menu, choose EC2.\n\nIn the left navigation pane, choose EC2 Dashboard to ensure that you are on the dashboard page.\n\nChoose Launch instance, and then select Launch instance.\n\n#### Naming your EC2 instance {#Naming-your-EC2-instance}\nWhen you name your instance, AWS creates a key-value pair. The key for this pair is Name, and the value is the name you enter for your EC2 instance.\n\nIn the Name and tags pane, in the Name text box, enter the name of your EC2 instance.\n\n#### Choosing an Amazon Machine Image (AMI) {#Choosing-an-Amazon-Machine-Image-(AMI)}\nAn AMI provides the information required to launch an instance, which is a virtual server in the cloud. An AMI includes the following:\n\nA template for the root volume for the instance (for example, an operating system or an application server with applications)\nLaunch permissions that control which AWS accounts can use the AMI to launch instances\nA block device mapping that specifies the volumes to attach to the instance when it is launched\nThe Quick Start list contains the most commonly used AMIs. You can also create your own AMI or select an AMI from the AWS Marketplace, an online store where you can sell or buy software that runs on AWS.\n\nLocate the Application and OS Images (Amazon Machine Image) pane.\n\nUnder AMI Machine Image (AMI), notice that the Amazon Linux 2 AMI image is selected by default. You can decide to keep this setting or choose your preferred.\n\n#### Choosing an instance type {#Choosing-an-instance-type}\nAmazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes so that you can scale your resources to the requirements of your target workload.\n\nFor example, a t3.micro instance has 2 virtual CPUs and 1 GiB of memory.\n\n#### Configuring a key pair {#Configuring-a-key-pair}\nAmazon EC2 uses public-key cryptography to encrypt and decrypt login information. To log in to your instance, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance.\n\nIt is an optional feature but mostly recommended.\n\n#### Configuring the network settings {#Configuring-the-network-settings}\nYou use this pane to configure networking settings.\n\nThe VPC indicates which virtual private cloud (VPC) you want to launch the instance into. You can have multiple VPCs, including different ones for development, testing, and production.\n\nIn the Network settings pane, choose Edit\n\nHere you can configure the VPC and Security Group.\n\nIn the security group, you give the name and the description.\n\nA security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you associate one or more security groups with the instance. You add rules to each security group that allow traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.\n\n#### Adding storage {#Adding-storage}\nAmazon EC2 stores data on a network-attached virtual disk called Amazon Elastic Block Store (Amazon EBS).\n\nYou launch the EC2 instance using a default 8 GiB disk volume. This is your root volume (also known as a boot volume).\n\nYou can do this in the Configure Storage pane.\n\n#### Configuring advanced details {#Configuring-advanced-details}\nIn the Advanced details pane, you can set advanced features like termination protection, among others.\n\nWhen you launch an instance in Amazon EC2, you have the option of passing user data to the instance. These commands can be used to perform common automated configuration tasks and even run scripts after the instance starts.\n\nFor example, in the User Data text box, you can define your scripts that will run when the instance launches.\n\n```\n#!/bin/bash\nyum -y install httpd\nsystemctl enable httpd\nsystemctl start httpd\necho '<html><h1>Hello From Your Web Server!</h1></html>' > /var/www/html/index.html\n```\nIn this case, the script does the following:\n\n* Install an Apache web server (httpd)\n\n* Configure the web server to automatically start on boot\n\n* Activate the Web server\n\n* Create a simple web page\n\n#### Launching an EC2 instance {#Launching-an-EC2-instance}\n\nNow that you have configured your EC2 instance settings, it is time to launch your instance.\n\nIn the right pane, choose Launch instance\n\nChoose **View all instances**\n\nThe instance appears in a Pending state, which means it is being launched. It then changes to Running, which indicates that the instance has started booting. There will be a short time before you can access the instance.\n\nThe instance receives a public DNS name that you can use to contact the instance from the Internet.\n\n### 2. Monitor Your Instance {#2.-Monitor-Your-Instance}\nMonitoring is an important part of maintaining the reliability, availability, and performance of your Amazon Elastic Compute Cloud (Amazon EC2) instances and your AWS solutions.\n\nSelect the instance by checking the box next to the instance and navigate to the bottom of the screen to the Status checks tab.\n\n> With instance status monitoring, you can quickly determine whether Amazon EC2 has detected any problems that might prevent your instances from running applications. Amazon EC2 performs automated checks on every running EC2 instance to identify hardware and software issues.\n\nSelect the **Monitoring tab**.\n\nThis tab displays Amazon CloudWatch metrics for your instance.\n\nYou can choose a graph to see an expanded view.\n\n> Amazon EC2 sends metrics to Amazon CloudWatch for your EC2 instances. Basic (five-minute) monitoring is enabled by default. You can enable detailed (one-minute) monitoring.\n\nIn the **Actions** menu, select Monitor and troubleshoot Get Instance Screenshot.\n\nThis shows you what your Amazon EC2 instance console would look like if a screen were attached to it.\n\n> If you are unable to reach your instance via SSH or RDP, you can capture a screenshot of your instance and view it as an image. This provides visibility as to the status of the instance, and allows for quicker troubleshooting.\n\n### 3. Update Your Security Group and Access the Web Server {#3.-Update-Your-Security-Group-and-Access-the-Web-Server}\nWhen we launched the EC2 instance, we provided a script that installed a web server and created a simple web page. In this section, we will access content from the web server.\n\nWhen we select the instance by checking the box, select the Details tab, and copy the Public IPv4 address of our instance to the clipboard.\n\nOpen a new tab in our web browser, paste the IP address we just copied, and then press Enter.\n\nWe are not currently able to access our web server because the security group is not permitting inbound traffic on port 80, which is used for HTTP web requests. This is a demonstration of using a security group as a firewall to restrict the network traffic that is allowed in and out of an instance.\n\nTo correct this, we will now update the security group to permit web traffic on port 80.\n\n* In the EC2 Management Console tab, in the left navigation pane, select Security Groups located under Network & Security.\n\n* Select Web Server security group. The Web in this case is the name we gave as our security group name.\n\n* Select the Inbound rules tab.\n\n* The security group currently has no rules.\n\n* Select Edit inbound rules then select Add rule and configure the rule with the following settings:\n\n* Type: HTTP\n\n* Source: Anywhere-IPv4\n\n* Select Save rules\n\nWhen we return to the web server tab that we previously opened and refresh the page.\n\nWe should see the message Hello From Your Web Server!\n\nWe can add, edit, or delete rules using the **Edit inbound rules** menu.\n\n### 4. Resize Your Instance: Instance Type and EBS Volume {#4.-Resize-Your-Instance:-Instance-Type-and-EBS-Volume}\nAs your needs change, you might find that your instance is over-utilized (too small) or under-utilized (too large). If so, you can change the instance type. For example, if a t3.micro instance is too small for its workload, you can change it to an m5.medium instance. Similarly, you can change the size of a disk.\n\n#### Stop Your Instance {#Stop-Your-Instance}\nBefore you can resize an instance, you must stop it.\n\nWhen you stop an instance, it is shut down. There is no charge for a stopped EC2 instance, but the storage charge for attached Amazon EBS volumes remains.\n\n* On the EC2 Management Console, in the left navigation pane, select Instances.\n* Web Server (the name of our instance) should already be selected.\n* Select Instance state > Stop instance.\n* Select Stop\n* Your instance will perform a normal shutdown and then will stop running.\n* Wait for the Instance State to display: stopped\n\n#### Change The Instance Type {#Change-The-Instance-Type}\nIn the Actions menu, select Instance Settings > Change Instance Type, then configure to the best of your choice.\n\nThen select Apply\n\nWhen the instance is started again, it will be of the new instance type you have configured above.\n\n### Resize the EBS Volume {#Resize-the-EBS-Volume}\n* In the left navigation menu, select Volumes located under Elastic Block Store.\n* Select the volume by checking the box, and navigate to the Actions menu, select Modify Volume.\n* Change the volume to your most suitable size.\n* Select Modify\n* Select Modify to confirm and increase or decrease the size of the volume.\n#### Start the Resized Instance {#Start-the-Resized-Instance}\nYou will now start the instance again, which will now have more or less memory and more or less disk space.\n\n### 5. Termination Protection {#5.-Termination-Protection}\nYou can delete your instance when you no longer need it. This is referred to as terminating your instance. You cannot connect to or restart an instance after it has been terminated.\n\n* Select your instance\n* In the Actions menu, select Instance settings > Change termination protection.\n* Check Enable followed by Save.\n\nWhen you now select your instance and then navigate to the top and select Instance state menu, select Terminate instance, *There is a message that says: On an EBS-backed instance, the default action is for the root EBS volume to be deleted when the instance is terminated. Storage on any local drives will be lost. It will ask if you are sure that you want to terminate the instance. You will be able to select the Terminate button.*\n\n\n\n",
  },
  {
    id: 14,
    title: "1. What is an Amazon Machine Image (AMI)?",
    excerpt: "--- Id: 1014 Title: Introduction to an Amazon Linux Amazon Machine Image (AMI) Author: Steve Tags: AWS Topic: Amazon Machine Image (AMI) Abstract: Amazon Machine Images (AMIs) are essential building blocks in Amazon Web Services (AWS) that enable users to deploy virtual machines quickly and efficiently. An AMI is a...",
    date: "2025-01-01",
    readTime: "3 min read",
    category: "General",
    content: "---\nId: 1014\nTitle: Introduction to an Amazon Linux Amazon Machine Image (AMI)\nAuthor: Steve\nTags: AWS\nTopic: Amazon Machine Image (AMI)\nAbstract: Amazon Machine Images (AMIs) are essential building blocks in Amazon Web Services (AWS) that enable users to deploy virtual machines quickly and efficiently. An AMI is a pre-configured template that contains the necessary operating system, application environment, and software packages required to launch an Amazon EC2 instance. By using AMIs, businesses and developers can streamline their cloud infrastructure, ensure consistency across deployments, and optimize scalability. This article explores the fundamentals of AMIs, their types, creation process, and best practices for managing them in AWS environments.\nHeaderImage: /BL-1014/header.webp\nisPublished: true\n---\n\n## 1. What is an Amazon Machine Image (AMI)?\nAn Amazon Machine Image (AMI) is a template that contains a software configuration (operating system, application server, and applications) required to launch an EC2 instance. AMIs are region-specific, meaning they are stored in a specific AWS region and can be used to launch instances in that region.\n\n### AMIs can be:\n\n* Pre-configured by AWS: AWS provides a variety of pre-configured AMIs for popular operating systems like Amazon Linux, Ubuntu, Windows, etc.\n\n* Custom-built by users: Users can create custom AMIs tailored to their specific needs, such as pre-installed software, configurations, or security settings.\n\n## 2. Benefits of Using AMIs\n* **Consistency:** AMIs ensure that every instance launched from the same image is identical, reducing configuration drift.\n\n* **Speed:** Launching instances from an AMI is faster than manually configuring each instance.\n\n* **Automation**: AMIs enable automation of infrastructure deployment using tools like AWS CloudFormation, Terraform, or custom scripts.\n\n* **Disaster Recovery**: AMIs can be used to create backups of your instances, enabling quick recovery in case of failures.\n\n* **Scalability**: AMIs make it easy to scale applications by launching multiple instances from the same image.\n\n## 3. Creating an AMI\n\n### Prerequisites\n* An AWS account with appropriate permissions.\n* An existing EC2 instance to create an AMI from.\n* AWS CLI installed and configured on your local machine.\n\n### Step-by-Step Guide to Creating an AMI\n1. Log in to the AWS Management Console and navigate to the EC2 dashboard.\n2. Select the instance you want to create an AMI from.\n3. Create the AMI:\n\n* Right-click the instance and select **\"Create Image\"**.\n* Provide a name and description for the AMI.\n* Configure additional settings like adding tags or enabling No Reboot (if applicable).\n\n* Click **\"Create Image\"**.\n\n### Automating AMI Creation with AWS CLI\n\n```\n#!/bin/bash\n\n# Variables\nINSTANCE_ID=\"i-0abcd1234efgh5678\"  # Replace with your instance ID\nAMI_NAME=\"My-Custom-AMI\"\nAMI_DESCRIPTION=\"AMI created from instance $INSTANCE_ID\"\n\n# Create AMI\nAMI_ID=$(aws ec2 create-image \\\n    --instance-id $INSTANCE_ID \\\n    --name \"$AMI_NAME\" \\\n    --description \"$AMI_DESCRIPTION\" \\\n    --no-reboot \\\n    --output text)\n\n# Check AMI creation status\naws ec2 wait image-available \\\n    --image-ids $AMI_ID\n\necho \"AMI created successfully with ID: $AMI_ID\"\n```\n\n## 4. Managing AMIs\n\n### Copying AMIs Across Regions\nAMIs are region-specific, but you can copy them to other regions using the AWS CLI:\n\n```\n#!/bin/bash\n\n# Variables\nSOURCE_AMI_ID=\"ami-0abcd1234efgh5678\"  # Replace with your source AMI ID\nSOURCE_REGION=\"us-east-1\"\nTARGET_REGION=\"us-west-2\"\n\n# Copy AMI to another region\nNEW_AMI_ID=$(aws ec2 copy-image \\\n    --source-image-id $SOURCE_AMI_ID \\\n    --source-region $SOURCE_REGION \\\n    --region $TARGET_REGION \\\n    --output text)\n\necho \"AMI copied to $TARGET_REGION with ID: $NEW_AMI_ID\",\n\n```\n### Deregistering AMIs\nWhen an AMI is no longer needed, you can deregister it to save costs:\n\n```\n#!/bin/bash\n\n# Variables\nAMI_ID=\"ami-0abcd1234efgh5678\"  # Replace with your AMI ID\n\n# Deregister AMI\naws ec2 deregister-image \\\n    --image-id $AMI_ID\n\necho \"AMI deregistered successfully: $AMI_ID\"\n\n```\n\nYou can as well use the console to deregister AMIs.\n* Navigate to Images and Click **AMIs**\n* On the top right, Click on **Actions**\n* Click Deregister AMI\n\n\n### Automating AMI Management with Scripts\nYou can automate AMI management tasks like cleanup of old AMIs using a script:\n\n```\n#!/bin/bash\n\n# Variables\nRETENTION_DAYS=30  # Number of days to retain AMIs\nOWNER_ID=\"123456789012\"  # Replace with your AWS account ID\n\n# Find and deregister old AMIs\nOLD_AMIS=$(aws ec2 describe-images \\\n    --owners $OWNER_ID \\\n    --query \"Images[?CreationDate<='$(date -d \"-$RETENTION_DAYS days\" +%Y-%m-%d)'].ImageId\" \\\n    --output text)\n\nfor AMI_ID in $OLD_AMIS; do\n    echo \"Deregistering AMI: $AMI_ID\"\n    aws ec2 deregister-image --image-id $AMI_ID\ndone\n\necho \"AMI cleanup completed.\"\n\n```\n\n## 5. Conclusion\nAmazon Machine Images (AMIs) are a powerful tool for managing and deploying EC2 instances in AWS. By understanding how to create, manage, and automate AMIs, you can streamline your infrastructure deployment, ensure consistency, and reduce operational overhead. Whether you're a beginner or an experienced AWS user, mastering AMIs is essential for building scalable and reliable cloud environments.\n\nWith the scripts provided in this article, you can start automating your AMI workflows and take full advantage of AWS's capabilities. **Happy cloud computing!**\n\n",
  },
  {
    id: 15,
    title: "1. What is Linux? {#1.-What-is-Linux}",
    excerpt: "--- Id: 1015 Title: AWS Introduction to Linux Author: Steve Tags: AWS Topic: Linux in AWS Abstract: Linux is the backbone of modern cloud computing, making it a critical skill for anyone pursuing a career in cloud technologies, including AWS. Whether you're an aspiring AWS Network Engineer or a cloud...",
    date: "2025-01-01",
    readTime: "4 min read",
    category: "General",
    content: "---\nId: 1015\nTitle: AWS Introduction to Linux\nAuthor: Steve\nTags: AWS\nTopic: Linux in AWS\nAbstract: Linux is the backbone of modern cloud computing, making it a critical skill for anyone pursuing a career in cloud technologies, including AWS. Whether you're an aspiring AWS Network Engineer or a cloud practitioner, understanding Linux is essential for managing servers, deploying applications, and automating tasks.\nHeaderImage: /BL-1015/header.webp\nisPublished: true\n---\n\n## 1. What is Linux? {#1.-What-is-Linux}\nLinux is an open-source, Unix-like operating system based on the Linux kernel. It was created by Linus Torvalds in 1991 and has since grown into a robust ecosystem supported by a global community of developers. Linux is known for its stability, security, and flexibility, making it a popular choice for servers, cloud computing, and development environments.\n\n### Key Features of Linux: {#Key-Features-of-Linux}\n1. **Open Source**: Linux is free to use, modify, and distribute.\n2. **Multi-User and Multi-Tasking**: Multiple users can run multiple processes simultaneously.\n3. **Security**: Linux has robust user permissions and built-in security features.\n4. **Customizability**: Users can customize Linux to suit their needs.\n\n## 2. Why is Linux Important for AWS and Cloud Computing? {#2.-Why-is-Linux-Important-for-AWS-and-Cloud-Computing}\nLinux is the backbone of many cloud computing platforms, including AWS. Here\u2019s why Linux is essential for cloud professionals:\n\n* **AWS Infrastructure**: Many AWS services, such as Amazon EC2, use Linux-based operating systems by default.\n* **Cost-Effective**: Linux is free, reducing the cost of running cloud infrastructure.\n* **Automation:** Linux supports scripting and automation, which are critical for managing cloud resources.\n* **Compatibility**: Most cloud tools and frameworks are designed to work seamlessly with Linux.\n* **Security & Permissions**: Linux provides robust security through file permissions and user management.\n\nFor example, when you launch an Amazon EC2 instance, you can choose from a variety of Linux-based AMIs (Amazon Machine Images), such as Amazon Linux, Ubuntu, or CentOS.\n\n* Amazon Linux \u2013 Optimized for AWS workloads.\n* Ubuntu \u2013 Popular for cloud applications.\n* CentOS/RHEL \u2013 Enterprise-grade server OS.\n* Debian \u2013 Known for stability and security.\n\n## 3. Key Concepts in Linux {#3.-Key-Concepts-in-Linux}\n\n### File System Hierarchy {#File-System-Hierarchy}\nLinux organizes files in a hierarchical directory structure. Here are some key directories:\n\n* /: The root directory.\n* /home: Contains user home directories.\n* /etc: Stores configuration files.\n* /var: Contains variable data like logs.\n* /bin: Contains essential binary files (executables).\n\n### Users and Permissions {#Users-and-Permissions}\nLinux is a multi-user system, and each user has specific permissions to access files and directories. Permissions are divided into three categories:\n\n* Owner: The user who owns the file.\n* Group: Users belonging to a specific group.\n* Others: All other users.\n\nExample\n\n```\n# Check file permissions\nls -l /home/user/file.txt\n\n# Change file permissions\nchmod 755 /home/user/file.txt  # Owner: read/write/execute, Group/Others: read/execute\n```\n### Processes and Services {#Processes-and-Services}\nLinux runs processes in the background (daemons) to manage services like web servers, databases, and networking.\n\nExample:\n\n```\n# List running processes\nps aux\n\n# Start a service\nsudo systemctl start apache2  # Start Apache web server\n\n```\n\n## 4. Real-Life Examples of Linux in AWS {#4.-Real-Life-Examples-of-Linux-in-AWS}\n\n#### Example 1 Launching an EC2 Instance with Amazon Linux {#Example-1-Launching-an-EC2-Instance-with-Amazon-Linux}\nWhen you launch an EC2 instance, you can choose an Amazon Linux AMI. Once the instance is running, you can connect to it via SSH and manage it using Linux commands.\n\n```\n# Connect to an EC2 instance\nssh -i my-key.pem ec2-user@<ip address of instance>\n\n```\n\n#### Example 2 Automating Backups with a Linux Script {#Example-2-Automating-Backups-with-a-Linux-Script}\nYou can use a Linux script to automate backups of your EC2 instance data to an S3 bucket.\n\n```\n#!/bin/bash\n\n# Variables\nBACKUP_DIR=\"/home/ec2-user/backups\"\nS3_BUCKET=\"s3://my-backup-bucket\"\nDATE=$(date +%Y-%m-%d)\n\n# Create backup\ntar -czf $BACKUP_DIR/backup-$DATE.tar.gz /var/www/html\n\n# Upload to S3\naws s3 cp $BACKUP_DIR/backup-$DATE.tar.gz $S3_BUCKET\n\necho \"Backup completed and uploaded to S3.\"\n\n```\n\n## 5. Basic Linux Commands and Scripts {#5.-Basic-Linux-Commands-and-Scripts}\nHere are some essential Linux commands and a sample script to get you started:\n\nCommon Commands\n\n```\n# Navigate directories\ncd /home/user\n\n# List files\nls -la\n\n# Create a file\ntouch file.txt\n\n# Create a directory\nmkdir /home/ec2-user/backups\n\n# Edit a file\nnano file.txt\nvi file2.txt\nvim file3.txt\n\n# View file content\ncat file.txt\n\n# copy files\ncp /var/log/myapp.log /home/ec2-user/backups/\n\n# Delete files and directories\nrm /home/ec2-user/backups/myapp.log\nrm -rf /home/backups\n\n# Search for text in a file\ngrep \"search-term\" file.txt\n\n# Compress files\ntar -czvf backup.tar.gz /var/www/html\n\n# Check disk usage\ndf -h\n\n```\n\nExample of a sript to monitor Disk Usage in AWS\n\n```\n#!/bin/bash\n\n# Variables\nTHRESHOLD=90  # Disk usage threshold in percentage\n\n# Check disk usage\nDISK_USAGE=$(df / | grep / | awk '{ print $5 }' | sed 's/%//g')\n\n# Compare with threshold\nif [ $DISK_USAGE -gt $THRESHOLD ]; then\n    echo \"Disk usage is above $THRESHOLD%. Current usage: $DISK_USAGE%\"\nelse\n    echo \"Disk usage is within limits. Current usage: $DISK_USAGE%\"\nfi\n\n```\n\n## Conclusion {#Conclusion}\nLinux is a powerful and versatile operating system that plays a critical role in cloud computing and AWS. Whether you\u2019re managing EC2 instances, automating backups, or troubleshooting network issues, Linux skills are essential for success in the cloud industry. The AWS re/Start program recognizes this and provides a strong foundation in Linux to prepare learners for cloud careers.\n\nBy mastering Linux, you\u2019ll not only enhance your technical skills but also open doors to exciting opportunities in the cloud ecosystem. So, start exploring Linux today, experiment with commands and scripts, and take your first step toward becoming a cloud professional!",
  },
  {
    id: 16,
    title: "Why Learn the Linux Command Line? {#Why-Learn-the-Linux-Command-Line}",
    excerpt: "--- Id: 1016 Title: Mastering the Linux Command Line Author: Steve Tags: AWS Topic: Linux in AWS Abstract: The Linux command line is the backbone of system administration, cloud computing, and DevOps. It\u2019s a powerful tool that allows users to interact with their operating system efficiently, automate tasks, and manage...",
    date: "2025-01-01",
    readTime: "4 min read",
    category: "General",
    content: "---\nId: 1016\nTitle: Mastering the Linux Command Line\nAuthor: Steve\nTags: AWS\nTopic: Linux in AWS\nAbstract: The Linux command line is the backbone of system administration, cloud computing, and DevOps. It\u2019s a powerful tool that allows users to interact with their operating system efficiently, automate tasks, and manage systems at scale. In the AWS re/Start program, Module 2 Linux Command Line is a critical component that equips learners with the foundational skills needed to work in cloud environments. This blog will provide a detailed exploration of the Linux command line, covering the key concepts taught in Module 2 and demonstrating how these skills can be applied in real-world scenarios.\nHeaderImage: /BL-1016/header.webp\nisPublished: true\n---\n\n## Why Learn the Linux Command Line? {#Why-Learn-the-Linux-Command-Line}\nBefore diving into the specifics, it\u2019s important to understand why the Linux command line is so essential:\n\n* **Ubiquity in Cloud Computing**: Most cloud servers run on Linux, and AWS is no exception. Knowing how to navigate and manage Linux systems is crucial for working with cloud infrastructure.\n\n* **Automation**: The command line allows you to automate repetitive tasks, saving time and reducing errors.\n\n* **Troubleshooting**: When things go wrong, the command line is often the fastest way to diagnose and fix issues.\n\n* **Flexibility**: The command line provides access to a wide range of tools and utilities that are not available in graphical interfaces.\n\n## What the Article covers {#What-the-Article-covers}\n* Describe the login workflow\u2022\n* Explain the Linux command syntax\u2022\n* Perform basic operations at the command line\u2022\n* Explain standard input, standard output, and standard error\n\n### Linux login workflow {#Linux-login-workflow}\nAfter a network connection is made, you can connect by using a program like Putty or by using the terminal on Mac OS. You will encounter the login prompt. All Linux sessions begin with the login process (default authentication process). Linux sessions start with the user entering their user name at the prompt. The login prompt is used to authenticate (prove the user\u2019s identity) before using a Linux system. When the password is typed, it does not echo (a line of text isn\u2019t displayed).\n\n\nThe user name is checked against the /etc/.psswdfile, which is stored in the /etcdirectory. The file represents an individual user account and contains the following fields separated by colons (:)\n\n1. User name or login name\n2. Encrypted password\n3. User ID\n4. Group ID5.User description\n6. User\u2019s home directory\n7. User\u2019s login shell\n\nDuring the login workflow, the name is checked against the `/etc/passwd` file, and the password is checked against the `/etc/shadow` file.\n\n```\n# User enters username:\n# The system checks /etc/passwd for the username.\nlogin: steve\n\n# User enters password\n# The system checks the hashed password stored in /etc/shadow.\n# If correct, it grants access and starts the user shell.\nPassword:\n\n\n[steve@hostname ~]$ pwd\n/home/steve\n[steve@hostname ~]$\n\n```\n\nCommand Prompt Components\nOnce authenticated, the user is presented with the command prompt, which consists of:\n\n* Username: Displays the logged-in user.\n* Hostname: Shows the system's name.\n* Current Directory (PWD): The directory where the user is located.\n* Prompt Symbol:\n- `$` \u2192 Regular user\n- `# `\u2192 Root user\n\n## Useful commands {#Useful-commands}\n### The whoami command {#The-whoami-command}\n\nIt is used to show the current user's user name. \n\nYou must ensure that you are the correct user invoking certain commands in the terminal and thus to confirm this, the `whoami` command is used.\n\n```\n[steve@hostname ~]$ whoami\nsteve\n[steve@hostname ~]$\n```\n\n### The id command {#The-id-command}\nThis command is used to identify user and group name and numeric IDs (group id)\nIt displays the user and the group information for each specified USER or (when the USER is omitted) for the current user.\n\n![whoami](/BL-1016/whoami.png)\n\n### The hostname command {#The-hostname-command}\nIt is used to set or display the system's current host, domain, or node name.\n\n![hostname](/BL-1016/hostname.png)\n\n### The `uptime` command {#The-uptime-command}\nIt indicates how long the system has been up since the last boot\n\n![uptime](/BL-1016/uptime.png)\n\n### The date command {#The-date-command}\nIt can set or dispaly rhe current time in a given format.\n\n![date](/BL-1016/date.png)\n\n### The cal command {#The-cal-command}\nThe command displays the calendar. If no argument is specified, the current month is displayed.\n\n```\n[steve@hostname ~]$ cal\n```\n\n### The clear command {#The-clear-command}\nThe command is used to clear the terminal screen. It clears all the text on the terminal screen.\n\n```\n[steve@hostname ~]$ clear\n```\n\n### The echo command {#The-echo-command}\nThe command places a specified text on the screen. Useful in scripts to provide users with information as the script runs.\n\n```\n[steve@hostname ~]$ echo \"Hello Linux\"\nHello Linux\n[steve@hostname ~]$\n```\n### The history command {#The-history-command}\nBash keeps a history of each user's commands in a file in the user's home directory.\n\nThe `history` command views the history file and displays the current user's history file.\n\n![history](/BL-1016/history.png)\n\n>Note: If you make a mistake when writing a command, don't reenter it. Use the\nhistory command to call the command back to the prompt, and then edit the\ncommand to correct the mistake\n>\n\n### The touch command {#The-touch-command}\n\nUsed to create, change, or modify timestamps on existing files.\n\nAlso used to create new empty files in a directory. `touch file_name`\n\nYou can create more than one new file `touch file1 file2 file3 filen`\n\n### The cat command {#The-cat-command}\nThis command is used to show the current contents of a file.\n\n![cat](/BL-1016/cat.png)\n\n\n#### There are thousands of commands, but the above are the most commonly used.\n\n",
  },
  {
    id: 17,
    title: "What is an AWS Elastic IP {#What-is-an-AWS-Elastic-IP}",
    excerpt: "--- Id: 1017 Title: AWS Elastic IP Author: Steve Tags: AWS Topic: Networking in AWS Abstract: In the world of cloud computing, managing network resources efficiently is crucial for ensuring high availability, scalability, and reliability. One such resource in Amazon Web Services (AWS) is the Elastic IP address. Elastic IPs...",
    date: "2025-01-01",
    readTime: "5 min read",
    category: "General",
    content: "---\nId: 1017\nTitle: AWS Elastic IP\nAuthor: Steve\nTags: AWS\nTopic: Networking in AWS\nAbstract: In the world of cloud computing, managing network resources efficiently is crucial for ensuring high availability, scalability, and reliability. One such resource in Amazon Web Services (AWS) is the Elastic IP address. Elastic IPs play a vital role in managing public IP addresses for your AWS resources, such as EC2 instances, NAT gateways, and more. This article will provide an in-depth exploration of AWS Elastic IPs, including their features, use cases, benefits, and step-by-step examples.\nHeaderImage: /BL-1017/header.png\nisPublished: true\n---\nThis article is based on static and dynamic IPs concept. A static IP address is an address that does not change even after a system reboot or stop.\n\nA dynamic IP address on the other hand is an address that changes anytime a system reboots or when the lease time is over. Another service called DHCP (Dynamic Host Configuration Protocol) manages this.\n\nThere are scenarios in which one would prefer either of the addresses depending on the type of service that is being utilized.\n\nSome services requires that the IP address should remain consistent all the time and thus a static IP address is preffered.\n\nTo achieve this, AWS offers a feature that enables static IP addressing for your instances or other supported resources in your account called Elastic IP.\n\n## What is an AWS Elastic IP {#What-is-an-AWS-Elastic-IP}\nAn Elastic IP (EIP) is a static, public IPv4 address designed for dynamic cloud computing. Unlike a traditional static IP address, an Elastic IP is associated with your AWS account, not a specific resource. This means you can allocate an Elastic IP and associate it with any EC2 instance, NAT gateway, or other supported resource in your account. If the resource is stopped or terminated, you can easily reassociate the Elastic IP with another resource.\n\nUnlike a regular public IP, which changes every time an instance is stopped and restarted, an Elastic IP remains static until you release it.\n\n## Why Use EIP {#Why-Use-EIP}\nElastic IPs are particularly useful in the following scenarios:\n\n* Hosting Public-Facing Applications: If you\u2019re running a web server, application server, or database that needs to be accessible over the internet, an Elastic IP ensures that the public IP address remains consistent.\n\n* Failover and High Availability: In case of an instance failure, you can quickly remap the Elastic IP to a standby instance, minimizing downtime.\n\n* NAT Gateways: Elastic IPs are required for NAT gateways to allow instances in a private subnet to access the internet.\n\n* DNS Configuration: Elastic IPs simplify DNS management since the IP address remains the same even if the underlying resource changes.\n\n## How Elastic IPs Work {#How-Elastic-IPs-Work}\nWhen you allocate an Elastic IP, AWS reserves a public IPv4 address for your account. You can then associate this IP address with an EC2 instance or other supported resource. Here\u2019s how it works:\n\n* Allocate: Reserve an Elastic IP address in your AWS account.\n\n* Associate: Attach the Elastic IP to an EC2 instance, NAT gateway, or other resource.\n\n* Reassociate: If the associated resource is stopped or terminated, you can reassign the Elastic IP to another resource.\n\n* Release: When you no longer need the Elastic IP, you can release it back to AWS.\n\n\n## The Key Characteristics of EIP\n* **Persistance** - The IP address remains associated with your account until you explicitly release it.\n* **Reusability** - You can remap the EIP to a different instance or resources as needed.\n* **Public Accessability** - EIP are designed for resources that need to be accessible over the internet, such as servers, databases, or APIs.\n\n## Importance of Amazon EIP\n\n#### Ensuring high availability and fault tolerance\nWhen instances fail due to maintenance, scaling or other operational needs, resulting into IP address changes. This can in turn lead to disruption of services and causing downtimes.\n\nWith an EIP, you can quickly remap the same IP to a new instance, ensuring minimal disruption and maintaining high availability for your application.\n\nFor example, if a web server hosting a critical application fails, you can launch a new instance and reassign the EIP to it, ensuring that users can continue accessing the application without noticing any change.\n\n#### Simplifying DNS Management\nInstead of updating DNS records every time an instance's public IP address changes, you can associate a domain name with an EIP.\n\n\n## Step-by-Step Guide to Using Elastic IPs {#Step-by-Step-Guide-to-Using-Elastic-IPs}\n\n\n### Step 1: Allocate an Elastic IP\n* Log in to the AWS Management Console.\n\n* Navigate to the EC2 Dashboard.\n\n* In the left-hand menu, under Network & Security, select E**lastic IPs**.\n\n* Click **Allocate Elastic IP address**.\n\n* Choose the scope (default is for use in EC2 instances) and click **Allocate**.\n\n### Step 2: Associate an Elastic IP with an EC2 Instance\n* In the **Elastic IPs** section, select the Elastic IP you just allocated.\n\n* Click **Actions** and choose **Associate Elastic IP address**.\n\n* In the **Resource type** dropdown, select **Instance**.\n\n* Choose the instance you want to associate the Elastic IP with.\n\n* Click **Associate**.\n\n### Step 3: Verify the Association\n* Go to the Instances section in the EC2 Dashboard.\n\n* Select the instance you associated the Elastic IP with.\n\n* Check the Public IPv4 address field. It should now display the Elastic IP.\n\n### Step 4: Reassociate an Elastic IP\nIf the associated instance fails or is terminated, you can reassociate the Elastic IP with another instance:\n\n* Go to the **Elastic IPs** section.\n\n* Select the **Elastic IP**.\n\n* Click **Actions** and choose **Associate Elastic IP address**.\n\n* Select the new instance and click **Associate**.\n\n### Step 5: Release an Elastic IP\nIf you no longer need the Elastic IP, you can release it:\n\n* Go to the **Elastic IPs** section.\n\n* Select the ***Elastic IP***.\n\n* Click **Actions** and choose **Release Elastic IP address**.\n\n* Confirm the release.\n-------------------------------------------------------------------------\n\n\n-------------------------------------------------------------------------\n\n## Conclusion\nAWS Elastic IPs are a powerful tool for managing public IP addresses in the cloud. They provide flexibility, high availability, and ease of use, making them ideal for hosting public-facing applications, configuring NAT gateways, and ensuring failover capabilities. By following best practices and understanding their limitations, you can leverage Elastic IPs to build resilient and scalable cloud architectures. Whether you\u2019re a beginner or an experienced AWS user, mastering Elastic IPs is a key step in optimizing your cloud infrastructure.\n\nWith the step-by-step examples and use cases provided in this guide, you\u2019re now equipped to start using Elastic IPs effectively in your AWS environment. Happy cloud computing!\n\n\n\n",
  },
  {
    id: 18,
    title: "Create Subnets and Allocate IP addresses in an Amazon Virtual Private Cloud (Amazon VPC)",
    excerpt: "--- Id: 1018 Title: Amazon VPC Author: Steve Tags: AWS Networks Topic: Networking in AWS Abstract: In today's cloud-driven world, Amazon Virtual Private Cloud (VPC) plays a crucial role in providing businesses with a secure, scalable, and customizable network environment within AWS. Amazon VPC allows users to create logically isolated...",
    date: "2025-01-01",
    readTime: "9 min read",
    category: "General",
    content: "---\nId: 1018\nTitle: Amazon VPC\nAuthor: Steve\nTags: AWS Networks\nTopic: Networking in AWS\nAbstract: In today's cloud-driven world, Amazon Virtual Private Cloud (VPC) plays a crucial role in providing businesses with a secure, scalable, and customizable network environment within AWS. Amazon VPC allows users to create logically isolated networks, define custom IP address ranges, manage subnets, and control traffic routing through Internet Gateways, NAT Gateways, and VPN connections. With fine-grained security controls, such as Security Groups and Network ACLs, Amazon VPC ensures that cloud resources remain protected while enabling seamless communication between services. This article explores the key components, benefits, and step-by-step setup of Amazon VPC, helping you design a robust network for your cloud applications.\nHeaderImage: /BL-1018/vpc.png\nisPublished: true\n---\n\n## Create Subnets and Allocate IP addresses in an Amazon Virtual Private Cloud (Amazon VPC)\n\n## An overview diagram of a VPC\n\n![vpc-diagram](/BL-1018/vpc.webp)\n\n\n## What is a VPC (Virtual Private Cloud)\nA VPC is like a data center but in the cloud. It is logically isolated from other virtual networks, and you can use a VPC to spin up and launch your AWS resources within minutes.\nResources within a VPC communicate with each other through private IP addresses. An instance needs a public IP address for it to communicate outside the VPC. The VPC needs networking resources, such as an internet gateway and a route table, for the instance to reach the internet.\nA CIDR block is a range of private IP addresses that is used within the VPC (for example, the /16 number that you see next to an IP address).\nA subnet is a range of IP addresses within your VPC.\n\n## Step by step into launching and configuring VPC\n\n### Step One Launching a VPC\n\n* **Define the VPC**: Specify the IP address range (CIDR block).\n* **Create Subnets**: Divide the VPC into public and private subnets.\n* **Configure Route Tables**: Define routes for traffic between subnets and to the internet.\n* **Attach an Internet Gateway**: Enable internet access for public subnets.\n* **Set Up NAT Gateway**: Allow private subnets to access the internet.\n* **Launch Resources**: Deploy EC2 instances, RDS databases, etc., in the appropriate subnets.\n* **Configure Security**: Use Security Groups and NACLs to control traffic.\n\n### Define the VPC\n 1. In the AWS console, type and search for VPC in the search bar on the top-left corner. Select VPC from the list. Alternatively, You can also find VPC under **Services** - **Networking & Content Delivery** in the top left corner\n\n![vpc-search](/BL-1018/vpc_search.png)\n\n\nYou are now in the Amazon VPC dashboard. You use the Amazon Virtual Private Cloud (Amazon VPC) service to build your VPC.\n\n2. Choose the Create VPC button to launch your first VPC. This will launch you into a step by step process to set up a VPC with it's basic components.\n\n![create-vpc](/BL-1018/create-vpc.png)\n\nYou can configure the following in order to lauch your VPC\nConfigure the following options:\n\n* VPC name - This is the name of the VPC\n\n* IPv6 CIDR block - Select an IPv4 Block(e.g 10.10.10/16).\n\nRecall that the CIDR block defines the size of the VPC in terms of the number of addresses that the VPC offers to devices.\n\n* Enable DNS hostnames and DNS resolutions if using public resources\n\n![create-vpc](/BL-1018/create-vpc2.png)\n\n![create-vpc](/BL-1018/create-vpc3.png)\n* Click **Create VPC**\n\nA VPC with the specified CIDR block has been created. Now, let's create the subnets.\n\n### Create Subnets (Public & Private)\n\n* In the left navigation pane, choose **Subnets** \u2192 **Create Subnet**.\n\n![create-vpc](/BL-1018/subnets.png)\n\n![create-vpc](/BL-1018/create-subnet.png)\n\n* Choose your VPC\n\n* Create a Public Subnet by stating the name, **CIDR block** of the subnet, **Availability Zone**, **Configure Auto-assign Public IP** as enable or disable and then click **Create**.\n\n>**Enable auto-assign public IPv4 address** provides a public IPv4 address for all instances\nlaunched into the selected subnet\n\n\n![create-vpc](/BL-1018/public_subnet.png)\n\n>A public subnet must have an internet\ngateway\n\n* Create a Private Subnet by stating the name, CIDR block of the subnet, Availability Zone, and disble Auto-assign Public IP and then click **Create**.\n\n![create-vpc](/BL-1018/private-subnet.png)\n\n\n![create-vpc](/BL-1018/public-subnet.png)\n\nYou can delete or edit the settings of a particular subnet by selecting the subnet in your dashboard then clicking on **Actions** and then select whether you want to delete or edit settings or manage the subnet.\n\n![edit-subnet](/BL-1018/edit-subnet1.png)\n\n![edit-subnet](/BL-1018/edit-subnet2.png)\n\n\n### Create Route Tables\n\n* In the VPC Dashboard, Click **Route Tables** \u2192 **Create Route Table**\n\n![create-routetable](/BL-1018/create-routet1.png)\n\n* Name your route table appropriately\n* Associate your the table with your VPC that you created\n* Click **Create route table**\n\n![create-routetable](/BL-1018/name-rt.png)\n\nYou can then add and edit routes \n* Click Edit routes\n\n![create-routetable](/BL-1018/edit-rt.png)\n\n![create-routetable](/BL-1018/edit-rt1.png)\n\n* Click **Save Changes**\n\nYou can associate your route table with your public subnet\n\n* Click on **Subnet Associations** tab\n* On the **Explicit subnet associations** click **Edit subnet associations**\n\n![create-routetable](/BL-1018/rt-assoc1.png)\n\n* Select the subnet you would like to associate with your route table\n* Click **Save Associations**\n\n![create-routetable](/BL-1018/rt-assoc2.png)\n\n## Attach an Internet Gateway {#Attach-an-Internet-Gateway}\n\nIn order for the resources in the public subnets to be able to access internet services or communication, the subnet is atttached to an Internet Gateway.\n\nAn Internet Gateway sits in between the VPC and the internet. Any communications from and to the internet must pass through this \"Gate\".\n\nIn order to create and attach an Internet Gateway in a VPC, the following steps are followed:\n\n* In the left navigation pane, choose **Internet Gateways** \u2192 **Create Internet Gateway**.\n\n![create-routetable](/BL-1018/go-to-igw.png)\n* Give a name to your Gateway e.g  `MyGateway` and a tag (optional)\n* Click **Create Internet Gateway**.\n\n![create-routetable](/BL-1018/create-igw.png)\n\n* Click **Attach to VPC**.  Alternatively, you can go to **Actions** and click **Attach to VPC**\n\n![create-routetable](/BL-1018/attach-igw.png)\n\n\n\n## Setting Up a NAT Gateway\nA NAT Gateway allows private subnet resources to be able to access the internet. The resources might need updates/patches hence they would download these patches from another server found in the internet.\n\nTo set up a NAT Gateway, the following steps are followed:\n\n* On the dashboard, Go to **NAT Gateways** -> **Create NAT Gateways**\n\n![create-routetable](/BL-1018/go-to-NAT.png)\n\n* Select the **Public Subnet** and create an **Elastic IP**\n\n* Click **Create NAT Gateway**\n\n![create-routetable](/BL-1018/create-NAT.png)\n\n* Go to **Route Tables**, create a **Private Route Table**, and add:\n\n    Destination: 0.0.0.0/0\n\n    Target: NAT Gateway\n\n![create-routetable](/BL-1018/edit-private-rt-for-NAT.png)\n\n![create-routetable](/BL-1018/edit-private-rt-for-NAT.png.png)\n\n* Associate the **Private Route Table** with the **Private Subnet**.\n\n![create-routetable](/BL-1018/private-NAT-assoc.png)\n\n![create-routetable](/BL-1018/private-NAT-save-assoc.png)\n\n## Create Security Groups\nSecurity groups are like firewalls at instance level. i.e. They control the traffic in and out of instances running in the VPC.\n\nThe following steps are followed when creating a security group:\n\n* Create a Security Group (e.g., WebServerSG).\n* Allow:\n\n    Inbound: HTTP (80), HTTPS (443), SSH (22) for Public Subnet.\n\n    Outbound: All traffic.\n* Associate this Security Group with your instances.\n\n### A simple VPC architecture\n```\nVPC: 10.0.0.0/16\n\u251c\u2500\u2500 Public Subnet (10.0.1.0/24) \n\u2502   \u251c\u2500\u2500 EC2 Instance (Public Web Server)\n\u2502   \u251c\u2500\u2500 Route Table (0.0.0.0/0 \u2192 IGW)\n\u2502   \u2514\u2500\u2500 Internet Gateway (IGW)\n\u251c\u2500\u2500 Private Subnet (10.0.2.0/24)\n\u2502   \u251c\u2500\u2500 EC2 Instance (Private App Server)\n\u2502   \u251c\u2500\u2500 Route Table (0.0.0.0/0 \u2192 NAT Gateway)\n\u2502   \u2514\u2500\u2500 NAT Gateway (for outbound internet access)\n\n```\n\n## Launch Resources {#Launch-Resources}\n\n* Go to EC2 \u2192 Launch Instance.\n* Choose an AMI (e.g., Amazon Linux 2).\n* Select the Public Subnet (for a web server).\n* Assign the Security Group you created.\n* Enable Auto-assign Public IP.\n* Click Launch.\n\n\n## The Key Components of a VPC {#The-Key-Components-of-a-VPC}\n\n### 1. IP Address range (CIDR Block) {#1.-IP-Address-range}\n\nA VPC is defined using a range of IP addresses.\n\nUnlike the traditional Classful IP addressing, which had limitations and wastage of IP addresses, the Classless Inter-Domain Routing makes it flexible to manage networks and IP addressing without limitations and wastage.\n\nThe CIDR determines the size of the VPC in terms of the number of IP addresses available in it.\n\n### 2. Subnets {#2.-Subnets}\n\nThis is a portion or segment of the VPC CIDR Block (IP address range) that one can place resources.\n\nA subnet is tied to a specific Availability Zone (AZ) within a region.\nThere are two types of subnets in a VPC:\n\n* Public Subnet - The resources in this subnet can access the internet using an internet gateway.(IGW)\n\n\n* Private Subnet - Resources in this subnet cannot directly access the internet. They can only do so via a NAT gateway or a NAT instance.\n\n### 3. Route Tables {#3.-Route-Tables}\n\nIn a house, we have rooms and for one to move from one room to another, he/she needs to follow certain paths or corridors in that house.\n\nIn a network, for a traffic to travel from one from a particular network to another, in order to reach the intended device, it needs to follow some paths.\n\nRouters are used to route (to provide the best paths) packets or traffic from one network to another.\n\nA route table contains a set og rules (routes) that determine where network traffic is directed.\n\nEach subnet must be associated with a route table.\n\nA default route is created automatically when the VPC is created. A default route is a route to destinations not found within the network.\n\n### 4. Internet Gateway(IGW) {#4.-Internet-Gateway}\nResources within the public or private networks of a VPC may want an internet access.\n\nIn order to communicate with the internet, a gateway (like a door) is required.\n\nAn internet gateway allows communication between resources in your public subnet of your VPC and the internet.\n\n### 5. NAT Gateway or NAT instance {#5.-NAT-Gateway-or-NAT-instance}\nThis gateway allowsresources within the private subnet to access the internet for updates or patches while preventing the internet from initiating connection to those Resources\n\nNAT Gateway is a managed service while a NAT instance is an EC2 instance configured to perform NAT.\n\n### 6. Security Groups {#6.-Security-Groups}\nJust like a traditional firewall that filters inbound and outbound traffic, a security group acts as a virtual firewall for instances in the VPC, to control inbound and outbound traffic.\n\nThey operate at instance level and are stateful (automatically allows return trffic).\n\nStateful means that they keep records of states or operations at the network interfaces. i.e. Memorize\n\n### 7. Network Access Control Lists (NACLs) {#7.-Network-Access-Control-Lists}\nThese are stateless(you must explicitly allow return traffic) firewall that operates at subnet level.\n\nThey deny or allow traffic based on defined rules.\n\n### 8. VPC Peering {#8.-VPC-Peering}\nVPC Peering allows you to connect two VPCs privately using AWS\u2019s network.\n\nThe VPCs can be in the same or different AWS accounts or regions.\n\nPeered VPCs can communicate as if they are part of the same network.\n\n### 9. VPC Endpoints {#9.-VPC-Endpoints}\nVPC Endpoints allow private connectivity between your VPC and supported AWS services (e.g., S3, DynamoDB) without using the internet or a NAT device.\n\nThere are two types:\n\n* Interface Endpoints: Uses an Elastic Network Interface (ENI) with a private IP address.\n\n* Gateway Endpoints: Uses a gateway for specific services like S3 and DynamoDB.\n\n### 10. Elastic IP Addresses {#10.-Elastic-IP-Addresses}\nAn Elastic IP (EIP) is a static, public IPv4 address that you can allocate to your AWS account and associate with instances or NAT Gateways.\n\n\n\n## Conclusion {#Conclusion}\nIn summary, a VPC is a foundational component of AWS that provides a secure, scalable, and customizable networking environment for your cloud resources. It is essential for building secure and efficient cloud architectures.\n",
  },
  {
    id: 19,
    title: "Introduction",
    excerpt: "--- Id: 1019 Title: Amazon Inspector Author: Steve Tags: AWS Topic: Security in AWS Abstract: In today\u2019s cloud-first world, security is non-negotiable. Amazon Inspector is AWS\u2019s automated security assessment service designed to help businesses identify vulnerabilities in their cloud environments. Whether you\u2019re a DevOps engineer, a security specialist, or an...",
    date: "2025-01-01",
    readTime: "2 min read",
    category: "General",
    content: "---\nId: 1019\nTitle: Amazon Inspector\nAuthor: Steve\nTags: AWS\nTopic: Security in AWS\nAbstract: In today\u2019s cloud-first world, security is non-negotiable. Amazon Inspector is AWS\u2019s automated security assessment service designed to help businesses identify vulnerabilities in their cloud environments. Whether you\u2019re a DevOps engineer, a security specialist, or an AWS user, understanding Amazon Inspector is key to maintaining compliance and reducing risks. This guide explores its features, use cases, best practices, and how it stacks up against competitors.\nHeaderImage: /BL-1019/AInspector.png\nisPublished: true\n---\n\n## Introduction\nIn today\u2019s cloud-first world, security is non-negotiable. Amazon Inspector is AWS\u2019s automated security assessment service designed to help businesses identify vulnerabilities in their cloud environments. Whether you\u2019re a DevOps engineer, a security specialist, or an AWS user, understanding Amazon Inspector is key to maintaining compliance and reducing risks. This guide explores its features, use cases, best practices, and how it stacks up against competitors.\n\n## **1. What is Amazon Inspector?**  \nAmazon Inspector is a **fully managed vulnerability assessment service** that automatically scans AWS workloads (EC2 instances, containers, Lambda functions, etc.) for security vulnerabilities, unintended network exposures, and deviations from best practices. It uses machine learning (ML) and AWS threat intelligence to prioritize risks.  \n\n## **2. Key Features and Capabilities**  \n- **Automated Scanning**: Continuously monitors EC2 instances, ECR container images, and serverless Lambda functions.  \n- **CVE Detection**: Identifies Common Vulnerabilities and Exposures (CVEs) like log4j, Heartbleed, and Shellshock.  \n- **Network Reachability Analysis**: Checks if vulnerabilities are exploitable via public/internet access.  \n- **Integration with AWS Services**: Works seamlessly with AWS Security Hub, Amazon EventBridge, and AWS Systems Manager.  \n- **Agentless Scanning (New in 2023)**: Scans EC2 instances without installing agents. \n\n## **3. How Amazon Inspector Works**  \n1. **Enable the Service**: Activate Amazon Inspector via AWS Management Console.  \n2. **Define Assessment Targets**: Select EC2 instances, containers, or Lambda functions.  \n3. **Automated Scanning**: Inspector analyzes:  \n   - Software vulnerabilities.  \n   - Network configurations (e.g., open ports).  \n   - Deviations from AWS security best practices.  \n4. **Generate Reports**: Detailed findings with severity scores (Low/Medium/High/Critical).  \n5. **Remediation**: Integrate with Systems Manager for patching or Jira for ticketing.  \n\n## **4. Use Cases**  \n- **DevSecOps**: Embed security into CI/CD pipelines.  \n- **Compliance Audits**: Prepare for PCI DSS or GDPR audits.  \n- **Container Security**: Scan ECR images before deployment.  \n- **Hybrid Cloud**: Assess on-premises servers via AWS Outposts.  \n- **Quickly discover zero-day vulnerabilities in compute workloads** - With inspector, one can automate discovery, expediate vulnerability routingc, and shorten MTTR with over 50 sources of vulnerability intelligence.\n- **Prioritize patch remediation** - Amazon Inspector uses the current CVE information and network accessibility to create contexual risk scores to prioritize and resolve vulnerable resources.\n\n## Getting Started With Amazon Inspector\n\n\n\n## **Conclusion**  \nAmazon Inspector is a game-changer for AWS users aiming to balance agility with security. By automating vulnerability assessments, it reduces human error and accelerates remediation. While it\u2019s not a silver bullet, combining it with AWS\u2019s security ecosystem creates a robust defense against evolving threats.  ",
  },
  {
    id: 20,
    title: "What is IAM",
    excerpt: "--- Id: 1020 Title: Amazon Identity and Access Management (IAM) Author: Steve Tags: AWS Topic: Security in AWS Abstract: AWS Identity and Access Management (IAM) is a powerful service that enables you to manage access to AWS services and resources securely. With IAM, you can create and manage AWS users...",
    date: "2025-01-01",
    readTime: "5 min read",
    category: "General",
    content: "---\nId: 1020\nTitle: Amazon Identity and Access Management (IAM)\nAuthor: Steve\nTags: AWS\nTopic: Security in AWS\nAbstract: AWS Identity and Access Management (IAM) is a powerful service that enables you to manage access to AWS services and resources securely. With IAM, you can create and manage AWS users and groups, and use permissions to allow or deny their access to AWS resources. IAM is a critical component of AWS security, and understanding how to use it effectively is essential for any organization using AWS. In this article, we will explore the key concepts of IAM, including users, groups, roles, policies, and permissions. We will also provide practical examples and code snippets to help you get started with IAM.\nHeaderImage: /BL-1020/IAM.png\nisPublished: true\n---\n\n## What is IAM\n\nIAM is a service that helps securely control access to AWS resources. You can use it to manage access to AWS services and resources securely.\n\nUsing IAM, you can create and manage AWS users and groups (to support authentication). You can also use IAM for permissions to allow or deny their access to AWS resources (to support authorization).\n\n- **Authentication**\n\nUse IAM to configure authentication, which is the first step because it controls who can access AWS resources. IAM is used for user authentication, and applications and other AWS services also use it for access.\n\n- **Authorization**\n\nIAM is used to configure authorization based on the user. Authorization determines which resources users can access and what they can do to or with those resources.\nAuthorization is defined through the use of policies. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.\n\n## Key Concepts of AWS IAM\n1. **Users**\n\n    An IAM user is an entity that you create in AWS to represent the person or application that interacts with AWS resources. Each user has a unique name and credentials (password or access keys) to access AWS services.\n\n    **IAM account root user**\n\n    When you first create an AWS account, you begin with a single sign-in identity. This entity has complete access to all AWS services and resources in the account and is called the AWS account root user. You access the account root user by signing in with the email address and password that you used to create the account.\n\n2. **Groups**\n\n    An IAM group is a collection of IAM users. You can use groups to specify permissions for multiple users, making it easier to manage permissions for users who have similar job responsibilities.\n\n3. **Roles**\n\n    An IAM role is an IAM identity that you can create in your account that has specific permissions. Unlike users, roles are not associated with a specific person or application. Instead, they are assumed by users, applications, or AWS services to gain temporary access to resources.\n\n4. **Policies**\n\n    An IAM policy is a document that defines permissions. Policies are written in JSON format and can be attached to users, groups, or roles to specify what actions are allowed or denied on which resources.\n\n5. **Permissions**\n\n    Permissions in IAM are defined by policies. They determine what actions a user, group, or role can perform on which AWS resources.\n\n6. **Identity federation**\n\n    This is a system of trust between two parties. Its purpose is to\n    authenticate users and convey the information needed to authorize their access to resources. In this system, an identity provider (IdP) is responsible for user authentication. A service provider (SP), such as a service or an application, controls access to resources.\n\n### IAM Policy Structure\nAn IAM policy is a JSON document that consists of one or more statements. Each statement includes the following elements:\n\n- **Effect**: Specifies whether the statement allows or denies access. Possible values are Allow or Deny.\n\n- **Action**: Specifies the AWS service actions that are allowed or denied.\n\n- **Resource**: Specifies the AWS resource(s) to which the actions apply.\n\n- **Condition** (optional): Specifies the conditions under which the policy is in effect.\n\nHere is an example of a simple IAM policy:\n\n```\n{\n  \"Version\": \"2025-2-20\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"arn:aws:s3:::example-bucket\"\n    }\n  ]\n}\n\n```\n\n## Creating IAM Users and Groups\n### Creating an IAM User\n\nYou can create an IAM user using the AWS Management Console, AWS CLI, or SDKs. Here\u2019s how to create an IAM user using the AWS CLI:\n\n```\naws iam create-user --user-name Steve\n```\n### Creating an IAM Group\nTo create an IAM group, use the following AWS CLI command:\n\n```\naws iam create-group --group-name Developers\n```\n### Adding a User to a Group\nTo add a user to a group, use the following command:\n\n```\naws iam add-user-to-group --user-name Steve --group-name Developers\n```\n\n### Attaching Policies to Users and Groups\n##### Attaching a Policy to a User\n\nTo attach a policy to a user, use the following command:\n\n```\naws iam attach-user-policy --user-name Steve --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n\n```\n##### Attaching a Policy to a Group\nTo attach a policy to a group, use the following command:\n\n```\naws iam attach-group-policy --group-name Developers --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess\n```\n\n## Creating and Using IAM Roles\n### Creating an IAM Role\nTo create an IAM role, you need to define a trust policy that specifies who can assume the role. Here\u2019s an example of a trust policy that allows an EC2 instance to assume the role:\n\n```\n{\n  \"Version\": \"2025-02-20\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n\n```\n\nYou can create the role using the AWS CLI:\n\n```\naws iam create-role --role-name EC2S3AccessRole --assume-role-policy-document file://trust-policy.json\n\n```\n\n#### Attaching a Policy to a Role\nTo attach a policy to the role, use the following command:\n\n```\naws iam attach-role-policy --role-name EC2S3AccessRole --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n\n```\n### Assuming a Role\nTo assume a role, you can use the AWS Security Token Service (STS) to request temporary security credentials. Here\u2019s an example using the AWS CLI:\n\n```\naws sts assume-role --role-arn arn:aws:iam::123456789012:role/EC2S3AccessRole --role-session-name MySessionName\n\n```\n\n## IAM Best Practices\n1. **Least Privilege**: Grant only the permissions required to perform a task. Avoid using overly permissive policies.\n\n2. **Use Groups**: Assign permissions to groups rather than individual users to simplify permission management.\n\n3. **Rotate Credentials Regularly**: Regularly rotate IAM user access keys and passwords to enhance security.\n\n4. **Enable MFA**: Enable Multi-Factor Authentication (MFA) for additional security, especially for privileged users.\n\n5. **Monitor Activity**: Use AWS CloudTrail to monitor and log IAM user activity.\n\n\n---\n\n## Conclusion\nAWS IAM is a fundamental service for managing access to AWS resources securely. By understanding and implementing IAM users, groups, roles, and policies, you can ensure that your AWS environment is secure and compliant with best practices. The examples and code snippets provided in this article should help you get started with IAM and integrate it into your AWS workflows.\n\nRemember, security is an ongoing process, and regularly reviewing and updating your IAM policies and permissions is crucial to maintaining a secure AWS environment.",
  },
  {
    id: 21,
    title: "What is AWS CloudTrail?",
    excerpt: "--- Id: 1021 Title: AWS CloudTrail Author: Steve Tags: AWS Security Governance Topic: Security in AWS Abstract: Unlocking the Power of AWS CloudTrail A Comprehensive Guide to Enhanced Cloud Security and Compliance... HeaderImage: /BL-1021/cloudtrail.jpg isPublished: true --- In today\u2019s rapidly evolving digital landscape, cloud computing has become the backbone of...",
    date: "2025-01-01",
    readTime: "3 min read",
    category: "General",
    content: "---\nId: 1021\nTitle: AWS CloudTrail\nAuthor: Steve\nTags: AWS Security Governance\nTopic: Security in AWS\nAbstract: Unlocking the Power of AWS CloudTrail A Comprehensive Guide to Enhanced Cloud Security and Compliance...\nHeaderImage: /BL-1021/cloudtrail.jpg\nisPublished: true\n---\n\nIn today\u2019s rapidly evolving digital landscape, cloud computing has become the backbone of modern businesses. As organizations increasingly migrate their operations to the cloud, ensuring security, compliance, and operational visibility has never been more critical. Enter AWS CloudTrail, a powerful service designed to provide unparalleled insight into user activity and resource changes across your AWS environment. In this article, we\u2019ll dive deep into what AWS CloudTrail is, how it works, and why it\u2019s an important tool for cloud governance.\n\n## What is AWS CloudTrail?\nAWS CloudTrail is a fully managed service that enables logging, monitoring, and auditing of all API calls made within your AWS account. Whether it\u2019s an action taken through the AWS Management Console, SDKs, command-line tools, or other AWS services, CloudTrail records every event, providing a comprehensive history of activity. This makes it an essential tool for security analysis, resource change tracking, and compliance auditing.\n\n## Key Features of AWS CloudTrail\n1. **Event History Logging**\n\n    CloudTrail automatically records API calls made in your AWS account, including details such as:\n\n    - The identity of the API caller\n    - The time of the API call\n    - The source IP address\n    - The request parameters\n    - The response elements returned by the service\n\n    This event history is stored in an S3 bucket and can be retained for up to 90 days by default, with options for longer retention using additional configurations.\n\n2. **Multi-Region and Multi-Account Support**\n\n    CloudTrail can be enabled across multiple AWS regions and accounts, ensuring centralized logging and visibility for organizations with complex cloud infrastructures.\n\n3. **Integration with Other AWS Services**\n\n    CloudTrail seamlessly integrates with services like AWS CloudWatch, AWS Lambda, and Amazon S3, enabling real-time monitoring, automated responses, and long-term storage of logs.\n\n4. **Security and Compliance**\n\n    By providing a detailed audit trail, CloudTrail helps organizations meet regulatory requirements such as GDPR, HIPAA, and PCI DSS. It also plays a crucial role in detecting unauthorized access or suspicious activity.\n\n5. **Event Insights**\n\n    CloudTrail offers insights events, which use machine learning to detect unusual API activity, such as spikes in resource deletions or failed login attempts, helping you identify potential security threats.\n\n## How Does AWS CloudTrail Work?\nWhen an action is performed in your AWS account, CloudTrail captures the event as a log entry. These logs are then delivered to an S3 bucket or CloudWatch Logs for storage and analysis. Here\u2019s a step-by-step breakdown of the process:\n\n- **Event Capture**: CloudTrail records API calls made by users, roles, or services.\n\n- **Log Delivery**: Logs are delivered to an S3 bucket or CloudWatch Logs.\n\n- **Storage and Analysis**: Logs can be analyzed using tools like Amazon Athena, AWS Lambda, or third-party SIEM solutions.\n\n- **Alerting and Automation**: CloudWatch Alerts or Lambda functions can trigger notifications or automated responses based on specific events.\n\n## Use Cases for AWS CloudTrail\n**1**. **Security Monitoring**\n\n    CloudTrail helps detect unauthorized access or changes to your AWS resources. For example, if an IAM user deletes a critical S3 bucket, CloudTrail logs the event, allowing you to investigate and take corrective action.\n\n**2**. **Operational Troubleshooting**\n\n    By reviewing CloudTrail logs, you can identify the root cause of operational issues, such as failed API calls or misconfigured resources.\n\n**3**. **Compliance Auditing**\n\n    CloudTrail provides the detailed audit trails required for compliance with industry standards and regulations. For instance, you can demonstrate who accessed sensitive data and when.\n\n**4**. **Resource Change Tracking**\n\n    CloudTrail logs every change made to your AWS resources, making it easy to track modifications and maintain an accurate inventory of your cloud environment.\n\n**5**. **Forensic Analysis**\n\n    In the event of a security incident, CloudTrail logs serve as a valuable resource for forensic analysis, helping you understand the scope and impact of the breach.\n\n## Conclusion\nAWS CloudTrail is an essential service for anyone using AWS, offering unmatched visibility into user activity and resource changes. By leveraging its robust logging and monitoring capabilities, organizations can enhance their security posture, streamline compliance efforts, and gain valuable insights into their cloud operations. Whether you\u2019re a small startup or a large enterprise, CloudTrail is a must-have tool for maintaining control and confidence in your AWS environment.\n\n",
  },
  {
    id: 22,
    title: "Introduction",
    excerpt: "--- Id: 1022 Title: Amazon CloudWatch Author: Steve Tags: AWS Monitoring Management Topic: Security in AWS Abstract: Managing and monitoring your resources on AWS is a crucial part of ensuring that your applications perform as expected. ou want to be able to monitor how your applications are being consumed, identify...",
    date: "2025-01-01",
    readTime: "11 min read",
    category: "General",
    content: "---\nId: 1022\nTitle: Amazon CloudWatch\nAuthor: Steve\nTags: AWS Monitoring Management\nTopic: Security in AWS\nAbstract: Managing and monitoring your resources on AWS is a crucial part of ensuring that your applications perform as expected. ou want to be able to monitor how your applications are being consumed, identify any technical issues that ...\nHeaderImage: /BL-1022/CloudWatch.png\nisPublished: true\n---\n## Introduction\nManagement and monitoring the resources on AWS is important. It helps in making sure that applications perform as expected, they are running in the most cost-efficient manner, secure and highly available.\n\nYou would also want to monitor the consumption of your application, and identify technicalities that may affect performance and availability.\n\nAs a management routine, you also need to be able  to audit your environment, with access to information such as access patterns, and identify anomalies that may indicate potential performance or security issues.\n\n## What is Amazon CloudWatch\nAmazon CloudWatch is a service that enables you to monitor your AWS resources and applications, running on AWS as well as on-premises in `real time.`\n\nWith CloudWatch, you can also collect resource and application `metrics`, `logs`, and `events`. These data can therefore help in analysis and identify trends.\n\nCloudWatch can also be used to configure `Alarms` that monitors metrics. If the metrics breaches certain thresholdsfor a specified period, an alarm is generated on which action can be taken to remediate.\n\nThe metrics can be either built-in or customed. Metrics exist in the region in which they are created, but you can also configure cross-account cross-region dashboards in order to gain visibility of those metrics,logs and alarms.\n\n\n## Amazon CloudWatch Use Cases\n1. **Infrastructure monitoring and troubleshooting**- With key metrics that can be collected, potential issues and bottlenecks can be identified. A root analysis can be conducted based on the data and resolution can be made.\n\n2. **Proactive resource optimization** - With alarms, metric values can be monitored and triggers can be set if breaches occur. An automatic remediation action can be defined for instance configuring auto-scaling, and terminating failed instances. When such happen a notification can be sent to admistrators and system operators.\n\n3. **Log analytics** - With the help of metrics and logs, the information obtained can be used to address operational issues, potential security attacks, or application performance issues, and take effective actions to remediate.\n\n## Dashboards\nDashboards are created on Amazon CloudWatch to allow visualization  and monitoring of resources and the metrics that are essential. They are configured to provide insights on resources' health across AWS accounts and Regions\n\nThis is an example of AWS CloudWatch Dashboared:\n\n![cloudwatchdashboard](/BL-1022/CloudWatch-dashboard.png)\n\n## Alarms\nAn alarm can be configured to monitor a given resource metric such as CPU utilization of an EC2 instance. If as set threshold for a specified timeis crossed, the alarm is triggered to make certain actions.\n\nFor example an alarm can be trigured when the average CPU utilization of an EC2 intance goes above 80% for 15 minutes. \n\nWhen an alarm is triggered,an automatic action such as Simple Notification Service (SNS) notification is taken to respond to it.\n\nAlarms can be in three states, `OK`, `Alarm` or `Insufficient data`. \n\n- `OK` - occurs when metric is within the range defined.\n- `Alarm` - occurs when a metric has breached a threshold for a period.\n- `Insufficient data` - Occurs when the data needed to make the decision is missing or incomplete.\n\n\n## Actions\nAre activities taken in response to an alarm.\nExamples include:\n\n- **Simple Notification Service (SNS) notification**: You can send out automatic alerts\nto an administrator (`application-to-person or A2P`) or push a notification to an\napplication to take some action (`application-to-application, or A2A`).\n\n- **Auto Scaling action**: The EC2 Auto Scaling service can be triggered to add or\nremove an EC2 instance in response to an alarm state.\n\n- **EC2 action**: You can have an alarm trigger an EC2 action, such as stopping an\nEC2 instance, terminating it, restarting it, or recovering it.\n\n## CloudWatch Logs\n\nLogs from both AWS sources such as EC2 instances, CloudTrail logs, Route 53 DNS queries, and VPC logs,  and non AWS sources such as your web application access logs, error logs, can be centrally collected and stored in Amazon CloudWatch.\n\nThese logs can be queried, analyzed, searched, and filtered for a specific pattern or error codes. They can also be visualized on the dashboard.\n\nLogs do not expire and therefore can be kept indefinitely. However, the retention policy can be adjusted by choosing the retention period between 1 day and 10 years. The logs can be archived into an Amazon S3 using the Glacier classes for long-term storage.\n\n## CloudWatch Events\n\nWith events, rules can be created to continously monitor AWS resources and then respond with an action when a given event occurs.\n\nAn event can be something like an EC2 instance entering the stop state because someone performed a shutdown operation on the instance, or when an IAM user logs into the AWS Management console.\n\nYou define a rule to perform an action that is taken when a certain event occurs to what target or resource.\n\nCloudWatch Events can help you respond to operational changes to complete workflows and task, or take any corrective action if required. CloudWatch events can also be used to schedule automated actions that trigger at certain times to help repeatable\nday-to-day operations. You utilize `standard rate` and `cron` expressions for this.\n\n\n## Let's do some Practical\n\n- **Create an Amazon SNS notification**\n- **Configure a CloudWatch alarm**\n- **Stress test an EC2 instance**\n- **Confirm that an Amazon SNS email was sent**\n- **Create a CloudWatch dashboard**\n\n## 1. Configure Amazon SNS\nIn this task, you create an SNS topic and then subscribe to it with an email address.\n\nRemember, Amazon **SNS** is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication.\n\nSteps:\n\n1. In the AWS Management Console, enter **SNS** in the search  bar, and then choose **Simple Notification Service**.\n\n    ![choosesns](/BL-1022/chooseSNS.png)\n\n2. On the left, choose the  button, choose Topics, and then choose **Create topic**.\n\n    ![createtopic](/BL-1022/create-topic1.png)\n\n3. On the Create topic page in the Details section, you can configure the following options:\n\n- Type (standard or FIFO)\n- Name e.g `MyAlarm`\n- Other oprtional features such as Encryption, Access policy, Tags etc\n\n    ![createtopic](/BL-1022/create-topic2.png)\n\n4. Choose Create topic.\n5. On the  `MyAlrm` details page, choose the Subscriptions tab, and then choose **Create subscription**.\n\n    ![createtopic](/BL-1022/create-subscription1.png)\n\n6. On the **Create subscription** page in the **Details** section, you can configure the following options:\n\n- Topic ARN: Leave the default option selected.\n- Protocol: From the dropdown list, choose Email.\n- Endpoint: Enter a valid email address that you can access.\n\n\n    ![createtopic](/BL-1022/create-sub2.png)\n\n7. Choose **Create subscription**. \n\n    In the **Details** section, the Status should be Pending confirmation. You should have received an AWS Notification - Subscription Confirmation email message at the email address that you provided in the previous step.\n\n8. Open the email that you received with the Amazon SNS subscription notification, and choose **Confirm subscription.**\n\n9. Go back to the AWS Management Console. In the left navigation pane, choose Subscriptions.\n\nThe Status should now be  **Confirmed**.\n\n![createtopic](/BL-1022/create-sub3.png)\n\n![createtopic](/BL-1022/create-sub4.png)\n\nIn this task, you created an SNS topic and then created a subscription for the topic by using an email address. This topic is now able to send alerts to the email address that you associated with the Amazon SNS subscription.\n\n\n## 2. Create a CloudWatch alarm\n\nIn this task, you view some metrics and logs stored within CloudWatch. You then create a CloudWatch alarm to initiate and send an email to your SNS topic if the Stress Test EC2 instance increases to more than 60 percent CPU utilization. \n\n>**Note**: CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), IT managers, and product owners. CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, and optimize resource utilization. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. You get a unified view of operational health and gain visibility of your AWS resources, applications, and services running on AWS and on premises.\n\nSteps\n\n1. In the AWS Management Console, enter Cloudwatch in the search  bar, and then choose it.\n\n    ![createtopic](/BL-1022/cloud-watch1.png)\n\n2. In the left navigation pane, choose the  **Metrics** dropdown list, and then choose **All metrics**.\n\n    ![createtopic](/BL-1022/cloud-watch2.png)\n\n3. On the Metrics page, choose EC2, and choose Per-Instance Metrics.\n\n    From this page, you can view all the metrics being logged and the specific EC2 instance for the metrics.\n\n4. Select the check box with CPUUtilization as the Metric name for the Stress Test EC2 instance.\n    ![createtopic](/BL-1022/cloud-watch3.png)\n\n5. In the left navigation pane, choose the **Alarms** dropdown list, and then choose **All alarms**.\n\n>You now create a metric alarm. A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. The action then sends a notification to the SNS topic that you created earlier.\n\n6. Choose Create alarm.\n\n7. Choose Select metric, choose EC2, and then choose Per-Instance Metrics.\n\n8. Select the check box with CPUUtilization as the Metric name for the Stress Test instance name.\n\n9. Choose Select metric.\n\n10. On the Specify metric and conditions page, you can configure the following options:\n\nMetric\n- Metric name: Enter `CPUUtilization`\n- InstanceId: Leave the default option selected.\n- Statistic: Enter `Average`\n- Period: From the dropdown list, choose `5` minutes.\n\nConditions\n- Threshold type: Choose `Static`.\n- Whenever CPUUtilization is...: Choose `Greater > threshold`.\n- than... Define the threshold value: Enter `60`\n\n    ![createtopic](/BL-1022/cloud-watch4.png)\n\n    ![createtopic](/BL-1022/cloud-watch5.png)\n\n11. Choose **Next**.\n\n12. On the Configure actions page, configure the following options:\n\n    Notification\n- Alarm state trigger: Choose In alarm.\n- Select an SNS topic: Choose Select an existing SNS topic.\n- Send a notification to...: Choose the text box, and then choose MyAlarm.\n\n    ![createtopic](/BL-1022/cloud-watch6.png)\n\n13. Choose Next, and then configure the following options:\n\n    Name and description\n- Alarm name: Enter CPUUtilizationAlarm\n- Alarm description - optional: Enter CloudWatch alarm for Stress Test EC2 instance CPUUtilization\n\n    ![watch7](/BL-1022/cloud-watch7.png)\n\n14. Choose **Next**\n\n15. Review the Preview and create page, and then choose **Create alarm**.\n\nIn this task, you viewed some Amazon EC2 metrics within CloudWatch. You then created a CloudWatch alarm that initiates an In alarm state when the CPU utilization threshold exceeds 60 percent. \n\n## 3. Test the Cloudwatch alarm\n\nIn this task, you log in to any of your EC2 instance and run a command that stresses the CPU load to 100 percent. This increase in CPU utilization activates the CloudWatch alarm, which causes Amazon SNS to send an email notification to the email address associated with the SNS topic.\n\n1. You can SSH into your instance and apply the following command to sress the instance i.e. to manually increase CPU load of the instance.\n\n    >Remember we created an alarm that would be triggered if the CPU utilization goes beyond 60%\n\n    ```\n    sudo stress --cpu 10 -v --timeout 400s\n    ```\n\n    This command runs for 400 seconds, loads the CPU to 100 percent, and then decreases the CPU to 0 percent after the allotted time.\n\n    ![stress1](/BL-1022/stress1.png)\n\n2. Navigate back to the AWS console where you have the CloudWatch Alarms page open.\n\n3. Choose **CPUUtilizationAlarm**.\n4. Monitor the graph while selecting the refresh button every 1 minute until the alarm status is In alarm.\n\n    ![stress5](/BL-1022/stress2.png)\n\n    ![stress4](/BL-1022/stress4.png)\n\n\n5. Navigate to your email inbox for the email address that you used to configure the Amazon SNS subscription. You should see a new email notification from **AWS Notifications**.\n\n    ![stress3](/BL-1022/stress3.png)\n\n\nIn this task, you ran a command to load the EC2 instance to 100 percent for 400 seconds. This increase in CPU utilization activated the alarm to go into the In alarm state, and you confirmed the spike in the CPU utilization by viewing the CloudWatch graph. You also received a email notification alerting you of the In alarm state.\n\n## 4. Create a CloudWatch dashboard\n\n> Remember CloudWatch dashboards are customizable home pages in the CloudWatch console that you can use to monitor your resources in a single view. With CloudWatch dashboards, you can even monitor resources that are spread across different Regions. You can use CloudWatch dashboards to create customized views of the metrics and alarms for your AWS resources.\n\n1. Go to the CloudWatch section in the AWS console. In the left navigation pane, choose **Dashboards**.\n\n2. Choose Create dashboard.\n3. Give a name to your Dashboard\n ![stressnotification](/BL-1022/dash5.png)\n4. You can choose from a variety of widgets, i.e Line, Number, Pie etc. Select the most suitable one for you.\n ![stressnotification](/BL-1022/dash2.png)\n5. Click Next\n6. A variety of metrics can be displayed on your dashboard. Select the most suitable.\n\n    In this case I will select EC2 and proceed with Per-Instance metrics\n\n    ![stressnotification](/BL-1022/dash3.png)\n7. Click Next\n8. Click Create widget\n    ![stressnotification](/BL-1022/dash4.png)\n\n9. Choose **Save** dashboard.\n\n    ![stressnotification](/BL-1022/dash6.png)\n\nNow you have created a quick access shortcut to view the CPUUtilization metric for the  instance.\n\n## Consclusion\n\nIn this article, we were able gain a deeper understanding on CloudWatch. Apart from the theortical work, the article went further to dive deep into hands-on practicals on how to:\n\n- Create an Amazon SNS notification\n- Configure a Cloudwatch alarm\n- Stress test an EC2 instance\n- Confirm that an Amazon SNS email was sent\n- Create a CloudWatch dashboard\n\nAmazon CloudWatch is more than just a monitoring tool\u2014it\u2019s your window into the health, performance, and efficiency of your AWS environment. By providing real-time insights, automated alerts, and actionable data, CloudWatch empowers you to proactively manage resources, optimize costs, and ensure seamless application performance. Whether you\u2019re tracking metrics, setting up alarms, or visualizing logs, CloudWatch simplifies the complexity of cloud operations.\n\nAs cloud environments grow, tools like CloudWatch become indispensable for maintaining visibility and control. Embrace CloudWatch to transform raw data into meaningful insights, and take your cloud management to the next level. With CloudWatch, you\u2019re not just monitoring your cloud\u2014you\u2019re mastering it.\n\n",
  },
  {
    id: 23,
    title: "Introduction",
    excerpt: "--- Id: 1023 Title: AWS Systems Manager Author: Steve Tags: AWS Monitoring Management Topic: Security in AWS Abstract: Managing and monitoring your resources on AWS is a crucial part of ensuring that your applications perform as expected. ou want to be able to monitor how your applications are being consumed,...",
    date: "2025-01-01",
    readTime: "12 min read",
    category: "General",
    content: "---\nId: 1023\nTitle: AWS Systems Manager\nAuthor: Steve\nTags: AWS Monitoring Management\nTopic: Security in AWS\nAbstract: Managing and monitoring your resources on AWS is a crucial part of ensuring that your applications perform as expected. ou want to be able to monitor how your applications are being consumed, identify any technical issues that ...\nHeaderImage: /BL-1023/AWS-SYSManager.png\nisPublished: true\n---\n\n## Introduction\n\nAs part of your day-to-day administrative task in maintaining your workloads, you want to be able to effectively manage your resources, such as patching, performing updates, and automating tasks.\n\n## What is AWS Systems Manager? \ud83c\udf4a\n\nAWS Systems Manager is a service that enables you to centrally manage yout AWS resources. You have the visibility of your resources across AWS services and you can perform configuration management, and automate day-to-day operational tasks.\n\nThe service utilizes the concept of documents(JSON and YAML), which define the actions that systems Manager performs on your managed resources such as operational management, change management, application management, and node management.\n\nThe documents can be pre-deined(e.g the AWS-CreateRdsSnapshot document that can be used to create RDS snapshot for an RDS instance.) or customized by the user.\n\n## Key Components of Patch Manager\n\n1. Patch Baseline \u2013 Defines which patches should be applied to instances based on categories like security updates, critical updates, and specific software versions.\n2. Patch Groups \u2013 Enables administrators to group instances together for easier patch deployment.\n3. Maintenance Windows \u2013 Schedules patching activities to run at predefined times to avoid disruption.\n4. Compliance Reporting \u2013 Provides reports on which instances are compliant or non-compliant based on applied patches.\n5. Patch Policies \u2013 Ensures updates align with organizational security and compliance standards.\n\n## What can you do with AWS Systems Manager?\n\n### 1. Run Command\nWith the run command, you are able to Keep your instances in a consistent state by enforcing security policies, firewall rules, and system settings automatically.\n\n### 2. State Manager\nWith state manager, you are able to configure a predefined state that can be used to maintain consistency in your configurations across a fleet of instances, such as firewall configurations, and antivirus configurations.\n\n### 3. Inventory\nThe inventory capability of AWS systems manager enables you to track applications, files, system configurations, and patches across your managed instances for better visibility and compliance.\n\n### 4. Maintenance Window\nThis service enables you to Schedule maintenance tasks (like patching or updates) during specific time windows so as not to disrupt normal business hours.\n\n### 5. Patch Manager\nAutomatically scan, approve, and install patches on your EC2 instances for improved security and compliance.\n\n### 6. Automation\nThis enables you to automate various maintenance tasks, such as updating AMIs(Amazon Machine Images), creating snapshots of EBS (Elastic Block Store) volumes, resetting passwords, and launching or terminating EC2 instances.\n\n### 7. Distributor\nWith Distributor, you can easily package, distribute, and update applications across multiple instances with seamless version control.\nYou can as well reinstall new package versions and perform in-place updates\n\n### 8. Session Manager\nWith Session Manager, you are able to remotely  access your EC2 instances using a browser-based shell or the CLI. It provides secure and auditable instance management without the need to open inbond ports, maintain bastion hosts, or manager SSH keys.\n\n### 9. Incident Manager\nThis capability enables you to manage and resolve incident affecting AWS-hosted applications. It offers a management console to track all your incidents and notify responders of impact, identify data that can help with troubleshooting, and help you get services back up and running\n\n### 10. Parameter Store\nStore and securely manage sensitive information like passwords and database connection strings, with encryption and access control.\n\n---\n\n## Patch Management\n\nAmong the many things you can do with AWS Systems Manager, I would like to pick on Patch Management. Let us look at how you can automate patches in Linux and Windows instances with a practical Scenario.\n\nIn organizations with hundreds and often thousands of workstations, it can be logistically challenging to keep all the operating system (OS) and application software up to date. In most cases, OS updates on workstations can be automatically applied via the network. However, administrators must have a clear security policy and baseline plan to ensure that all workstations are running a certain minimum version of software.\n\n### Steps for Implementing Patch Management in AWS Systems Manager\n\n\nAssuming you have a fleet of Linux and Windows Instance, In this simple exercise, we shall:\n- Patch Linux instances using default baseline\n- Create custom patch baseline \n- Use patch groups to to patch Windows instances using custom patch baseline \n- Verify patch compliance\n\n## 1. Patch Linux instances using default baselines\nIn this task, you patch Linux EC2 instances using default baselines available for the OS.\n\n>Patch Manager provides predefined patch baselines for each of the operating systems that it supports. You can use these baselines as they are currently configured (you can't customize them), or you can create your own custom patch baselines. You can use custom patch baselines for greater control over which patches are approved or rejected for your environment.\n\nSteps:\n\n#### 1. In the AWS Management Console, in the search  box, enter `Systems Manager` and select it. This option takes you to the Systems Manager console page.\n\n![sysmanager](/BL-1023/sysmanager1.png)\n\n#### 2. In the left navigation pane, under Node Tools, choose Fleet Manager.\n\n![fleet1](/BL-1023/fleet1.png)\n\nFor example In my case I have some pre-configured EC2 instances. There are three Linux instances and three Windows instances. These EC2 instances have a specific IAM role associated with them that allows you you managed them using Systems Manager, which is why you can view them in the Fleet Manager section.\n\n![fleet2](/BL-1023/fleet2.png)\n\nIf you Select one of the linux instances, you can view details about the specific instance, such as Platform type, Node type, OS name, and the IAM role that allows you to use Systems Manager to manage this instance.\n\n![fleet3](/BL-1023/fleet3.png)\n\n#### 3. At the top of the page, choose AWS Systems Manager to go back to the Systems Manager homepage.\n\n#### 4. In the left navigation pane, under **Node Tools**, choose **Patch Manager**.\n\n![patch1](/BL-1023/patch1.png)\n\nHere, you will find five tabs in the dashboard giving you an overview of Patch Management:\n\n- Dashboard \u2013 Provides an overview of patching compliance.\n- Compliance Reporting \u2013 Displays compliance status of EC2 instances.\n- Patch Baselines \u2013 Manages predefined patch rules for different OS types.\n- Patches \u2013 Lists available patches for EC2 instances.\n- Settings \u2013 Configure settings for Patch Manager.\n\n#### 5. Choose Patch now to patch the Linux instances with **AWS-AmazonLinux2DefaultPatchBaseline**.\n\nYou are presented with a Basic configuration window in which allows you to define how patches are applied to EC2 instances. The following are the configurations you can make:\n\n**Patching Operation**\n- Scan \u2192 Checks for missing patches but does NOT install them.\n- Scan and Install (Selected) \u2192 Checks for missing patches and installs them.\n\n**Reboot Option**\n- Reboot if needed (Selected) \u2192 If a patch requires a restart, the instance will be rebooted automatically.\n- Do not reboot my instances \u2192 Instances will not be restarted, even if required.\n- Schedule a reboot time \u2192 Allows scheduling a reboot at a specific time.\n\n**Instances to Patch**\n- Patch all instances \u2192 Applies patches to all managed instances.\n- Patch only the target instances I specify \u2192 Lets you select specific instances.\n\n**Patching Log Storage**\n- Logs from the patching process can be stored in an S3 bucket for auditing and troubleshooting.\n\n**Advanced Options**\n\nThis section is for configuring more complex patching workflows.\n\nLifecycle Hooks (Optional)\nUse lifecycle hooks \u2192 Allows executing SSM documents before and after patching.\nCreate SSM Document \u2192 Allows defining custom pre- and post-patch actions.\n\nFor example, for my case I will configure the following\n- Patching operation:  Scan and install\n- Reboot option: Reboot if needed\n- Instances to patch: Patch only the target instances I specify\n- Target selection: Specify instance tags\n\n    Tag key:  Patch Group\n\n    Tag value: LinuxProd\n    \n\n![patch2](/BL-1023/patch2.png)\n\n#### 6. Choose **Patch now**\n\n\nA new page displays. In the Association execution summary, there is a Status field that will show that three instances will be affected and the progress made.\n    ![patch3](/BL-1023/patch3.png)\n\n\n---\n\n## 2. Create a custom patch baseline for Windows instances\n>Although Windows has default patch baselines that you can use, we can also set up a baseline for Windows security updates. Remember you can customize your on patch baselines that you can use.\n\n>A patch baseline is a set of rules that define how patches are approved or rejected for specific operating systems and applications.\n\n#### 1. Return to the Systems Manager console. In the search bar at the top, enter Systems Manager and then select it.\n\n#### 2. In the left navigation pane, under Node Tools, choose **Patch Manager**. \n\n#### 3. Choose the **Patch baselines** tab.\n\n#### 4. Choose the **Create patch baseline** button. \n\n![create-patch](/BL-1023/create-patchbase1.png)\n\nHere is the Breakdown of the Patch Baseline Configuration:\n\nPatch Baseline Details\n- Name \u2192 The name of the patch baseline.\n- Description (Optional) \u2192 A short description of the patch baseline (not yet filled in).\n- Operating System \u2192  This patch baseline applies to the particular instances running the selected OS.\n- Default Patch Baseline (Checkbox) \u2192 If selected, this becomes the default patch baseline for Windows instances.\n\n\nApproval Rules for Operating Systems\n\nThis section defines the criteria for automatic patch approval.\n\nOperating System Rule 1\n- Products \u2192 Specifies which Windows products (e.g., Windows Server 2019, Windows 10) should receive patches.\n- Approval Method:\n    - Approve patches after a specified number of days \u2192 Automatically approves patches after the defined period.\n    - Approve patches released on a specific date \u2192 No specific date is set.\n\n- Classification \u2192 Defines the type of patches to approve (e.g., Security updates, Critical updates)\n\n- Severity \u2192 Determines the severity level of patches (e.g., Critical, Important)\n- Compliance Reporting (Optional) \u2192 If selected, patches will be reported for compliance\n- Additional rules can be added using the \"Add rule\" button.\n\nI will configure the following as an example:\n\nFor Patch baseline details:\n- Name, `WindowsServerSecurityUpdates`\n- For Description - optional, `Windows security baseline patch`\n- For Operating system, `Windows`.\n- I will Leave the check box for Default patch baseline unselected.\n\nIn the Approval rules for operating systems section:\n\n- Products: From the dropdown list,  `WindowsServer2019`. Also, deselect All so that it no longer appears under Products.\n\n- Severity: This option indicates the severity value of the patches that the rule applies to. To ensure that all service packs are included by the rule, I will choose `Critical` from the dropdown list.\n\n- Classification: From the dropdown list, `SecurityUpdates`.\n\n- Auto-approval: `3 days`.\n\n- Compliance reporting - optional: I will select `Critical`.\n\n![create-patch](/BL-1023/create-patchbase2.png)\n\n![create-patch](/BL-1023/create-patchbase3.png)\n\nI can add another rule by choosing the Add rule button and configure the following:\n\n![create-patch](/BL-1023/create-patchbase4.png)\n\n\n#### 5. Choose **Create patch baseline**.\n\n#### 6. Next, We can modify a patch group for the Windows patch baseline that we have just created, to associate it with a patch group.\n\n#### 7. In the Patch baselines section, select the button for the WindowsServerSecurityUpdates patch baseline.\n\n\n#### 8. Choose the Actions dropdown list, and then choose Modify patch groups.\n\n#### 9. In the Modify patch groups section under Patch groups, enter WindowsProd \n\n#### 10. Choose the Add button, and then choose Close. \n\n![create-patch](/BL-1023/create-patchbase5.png)\n\n## 3. Patching the Windows instances\n\nNow that we have created a patch baseline, we can use this baseline to patch our fleet of windows instances at once using the tag associated with the instances for example WindowsProd\n\n>After configuration, Patch Manager uses the Run Command to call the RunPatchBaseline document to evaluate which patches should be installed on target instances according to each instance's operating system type directly or during the defined schedule (maintenance window).\n\nWe can begin by tagging Windows instances so that while applying patches, the instances with a certain tag are targeted.\n\n### 3.1 Tagging Windows instances\n\n- In the AWS Management Console, in the search  bar, enter EC2 and select it.\n- Choose Instances, select the Windows instance you would want to tag, and then choose the Tags tab.\n- Choose the Manage tags button, choose Add new tag, and configure the following options:\n    - Key: Enter Patch Group\n\n    - Value: Enter WindowsProd\n\n- Choose Save.\n\n![create-patch](/BL-1023/create-patchbase7.png)\n\n![create-patch](/BL-1023/create-patchbase6.png)\n\nRepeat the above four steps for all the Windows instances tha you would want to tag in order to be patched.\n\n\n### 3.2 Patching Windows instances\n\n1. Return to the Systems Manager console. In the search bar at the top, enter Systems Manager and then select it.\n\n2. To Patch the Windows instances:\n\n- Choose Patch Manager \n\n- Choose Patch now.\n\n- Patching operation:  Scan and install\n\n- Reboot option: Reboot if needed\n\n- Instances to patch: Patch only the target instances I specify\n\n- Target selection: Specify instance tags\n\n    - Tag key:  Patch Group\n\n    - Tag value: WindowsProd\n\n    - Choose Add\n\n- Choose Patch now\n\n![create-patch](/BL-1023/create-patchbase8.png)\n![create-patch](/BL-1023/create-patchbase9.png)\n\n\nA new page displays. When it becomes available, choose the link to the Execution ID.\n\nA page in the State Manager part of Systems Manager opens.\n\nChoose the Output link for one of the managed instances that shows a status of InProgress.\n\nA page in the Run Command part of Systems Manager opens.\n\nExpand the Output panel to observe the details.\n![create-patch](/BL-1023/create-patchbase10.png)\n\n >Behind the scenes, Patch Manager uses the Run Command to run the PatchBaselineOperations.  If you scroll through the output, you should see the PatchGroup: WindowsProd details.\n\n![create-patch](/BL-1023/create-patchbase11.png)\n\n>A Systems Manager document (SSM document) defines the actions that Systems Manager performs on your managed instances.\n\n## 4. Verifying compliance\nIn the left navigation pane, under Node Tools, choose Patch Manager.\n\nChoose the Dashboard tab. Under Compliance summary, you should now see Compliant:  which verifies that all Windows and Linux instances are compliant. This will depend on the number of instances you have running. In this case, I had 6 instances(3 Windows and 3 Linux) as shown in the image below.\n\n![create-patch](/BL-1023/compliant1.png)\n\nChoose the Compliance reporting tab. \n This tab provides an overview of all running instances with **SSM**. You should be able to verify that the Compliance status of all Linux and Windows instances is Compliant.\n\n![create-patch](/BL-1023/compliant2.png)\n\nNode patching details for each instance includes the following:\n\n - Critical noncompliant count\n- Security noncompliant count\n- Other noncompliant count\n- Last operation date \n- Baseline ID \n\n\n---\n## Conclusion\n\nIn this article we were able to have a glimpse of AWS Systems Manager. We saw what one is capable of doing with AWS Systems Manager. We went further and picked on one of the capabilities called Package Manager and we were able to see how this can be done through a practical exercise. AWS Systems Manager is a game-changer for managing and automating your AWS infrastructure. I hope you enjoyed the read!!",
  },
  {
    id: 24,
    title: "Amazon Key Management Service (KMS): A Comprehensive Guide with Hands-On Lab",
    excerpt: "--- Id: 1024 Title: AWS Key Management Service (KMS) Author: Steve Tags: Data Protection Encryption Topic: Security in AWS Abstract: Information stored in the cloud needs to be protected against unauthorized access, modification or delition. One of the ways to do this is by ensuring that data confidentiality is maintained....",
    date: "2025-01-01",
    readTime: "10 min read",
    category: "General",
    content: "---\nId: 1024\nTitle: AWS Key Management Service (KMS)\nAuthor: Steve\nTags: Data Protection Encryption\nTopic: Security in AWS\nAbstract: Information stored in the cloud needs to be protected against unauthorized access, modification or delition. One of the ways to do this is by ensuring that data confidentiality is maintained. This can be done through cryptography, a practice of converting information into secret code ...\nHeaderImage: /BL-10244/KMS.jpeg\nisPublished: true\n---\n\n# Amazon Key Management Service (KMS): A Comprehensive Guide with Hands-On Lab\n\nCryptography is the practice of converting information into secret code to ensure confidentiality and privacy. It plays a vital role in securing digital communications by preventing unauthorized access to sensitive data. Key functions of cryptography include authentication, data integrity, and nonrepudiation, with encryption serving as its central mechanism.\n\nEncryption transforms readable data into an unreadable format, making it accessible only to those with the proper decryption key. This process ensures that information remains protected from unauthorized individuals, safeguarding sensitive data in various applications, from secure messaging to financial transactions. As the digital landscape evolves, encryption continues to be a cornerstone of cybersecurity, enabling organizations and individuals to protect their information from potential threats.\n\nAmazon Key Management Service (KMS) is a fully managed service that makes it easy to create and control cryptographic keys used to encrypt data. It is a critical component of AWS's security and compliance offerings, enabling organizations to protect sensitive data, meet regulatory requirements, and implement robust encryption strategies. In this article, we\u2019ll dive deep into Amazon KMS, explore its features, and walk through a hands-on lab to demonstrate its capabilities. We\u2019ll also include facts, statistics, and expert insights to provide a well-rounded understanding of the service.\n\n---\n\n## 1. What is Amazon KMS?\n\nAmazon KMS is a scalable and secure key management service that integrates seamlessly with other AWS services and applications running on AWS. It allows you to create, manage, and control encryption keys used to encrypt your data. KMS supports symmetric and asymmetric keys, and it integrates with AWS services like S3, EBS, RDS, Lambda, and more.\n\n### Key Features of Amazon KMS\n- **Centralized Key Management**: Create, manage, and control cryptographic keys in a centralized manner.\n- **Integration with AWS Services**: Encrypt data stored in AWS services like S3, EBS, RDS, and DynamoDB.\n- **Custom Key Stores**: Use your own Hardware Security Modules (HSMs) to store keys.\n- **Auditing and Compliance**: Monitor key usage with AWS CloudTrail and meet compliance requirements.\n- **Scalability**: Automatically scales to meet your organization's needs.\n- **Granular Access Control**: Define fine-grained permissions using AWS Identity and Access Management (IAM) policies.\n\n---\n\n## 2. Why Use Amazon KMS?\n\n### Security\n- **Encryption at Rest and in Transit**: KMS ensures that your data is encrypted both at rest and in transit, protecting it from unauthorized access.\n- **HSM-Backed Keys**: Keys are protected by FIPS 140-2 validated HSMs, ensuring the highest level of security.\n\n### Compliance\n- KMS helps organizations meet compliance requirements such as GDPR, HIPAA, PCI-DSS, and SOC.\n\n### Ease of Use\n- KMS integrates seamlessly with AWS services, making it easy to encrypt data without managing complex infrastructure.\n\n### Cost-Effective\n- Pay only for what you use, with no upfront costs or long-term commitments.\n\n---\n\n## 3. Amazon KMS in Action: A Hands-On Lab\n\nIn this lab, we\u2019ll walk through the process of creating a KMS key, encrypting and decrypting data, and integrating KMS with an AWS service (S3).\n\nObjectives:\n- Create an AWS KMS encryption key\n- Install the AWS Encryption CLI\n- Encrypt plaintext\n- Decrypt ciphertext\n\n### Lab Setup\n1. **Prerequisites**:\n   - An AWS account.\n   - Basic knowledge of AWS services (IAM, S3).\n   - AWS CLI installed and configured.\n\n2. **Step 1: Create a KMS Key**\n   - Go to the AWS Management Console.\n    ![kms](/BL-10244/278-1.png)\n   - Navigate to **KMS > Customer managed keys > Create key**.\n    ![kms-create](/BL-10244/278-2.png)\n   - Choose **Symmetric** as the key type.\n    ![kms-keytype](/BL-10244/278-3.png)\n   - On the Add labels page, you can add a label and a decription toyour key as follows:\n    ![kms-addlabel](/BL-10244/278-4.png)  \n   - Define key usage permissions using IAM policies.\n    ![kms-permissions](/BL-10244/278-5.png)\n    ![kms-permissions](/BL-10244/278-6.png)\n   - Complete the key creation process.\n    ![kms-finish](/BL-10244/278-7.png)\n    ![kms-finish](/BL-10244/278-8.png)\n\n\n3. **Step 2: Configure the File Server instance** \n   - Before you can encrypt and decrypt data, you need to set up a few things. To use your AWS KMS key, you will configure AWS credentials on the File Server EC2 instance. After that, you will install the AWS Encryption CLI (aws-encryption-cli), which you can use to run encrypt and decrypt commands.\n   - The AWS credentials file is used to store authentication details for accessing AWS services securely via the AWS CLI (Command Line Interface), SDKs, and other AWS tools. It eliminates the need to manually enter credentials for every request and ensures secure access to AWS resources.\n    \n   - In the console, enter EC2 in the search  bar, and then choose EC2.\n    ![ec2](/BL-10244/278-11.png)\n   - In the Instances list, select the check box next for the instance you wish to connect to, in this case I have a File server instance, and then choose Connect. \n    ![ec2](/BL-10244/278-22.png)\n   - Choose the mathod of connection e.g Session Manager, and then choose Connect.\n    ![ec2](/BL-10244/278-33.png)\n   - To change to the home directory and create the AWS credentials file, run the following commands:\n        ```bash\n        cd ~\n        aws configure\n        ```\n        ![ec2](/BL-10244/278-44.png)\n   - To open the AWS credentials file, run the following command:\n        ```bash\n        vi ~/.aws/credentials\n        ```\n        ![ec2](/BL-10244/278-55.png)\n\n        This is an example of a credentials file structure:\n        ```ini\n        [default]\n        aws_access_key_id = AKIAEXAMPLE123456\n        aws_secret_access_key = abcdefghijklmnopqrstuvwxyz1234567890\n\n        [dev]\n        aws_access_key_id = AKIADEVEXAMPLE98765\n        aws_secret_access_key = zyxwvutsrqponmlkjihgfedcba0987654321\n        region = us-west-2\n\n        ```\n    - To install the AWS Encryption CLI and set your path, run the following commands:\n        ```bash\n        pip3 install aws-encryption-sdk-cli\n        export PATH=$PATH:/home/ssm-user/.local/bin \n        ```\n    \n4. **Step 3: Encrypt and Decrypt Data Using the KMS Key**\n   - Use the AWS CLI to encrypt a plaintext file:\n     ```bash\n     aws kms encrypt --key-id <your-key-id> --plaintext fileb://plaintext.txt --output text --query CiphertextBlob > encrypted.txt\n     ```\n   - Decrypt the encrypted file:\n     ```bash\n     aws kms decrypt --ciphertext-blob fileb://encrypted.txt --output text --query Plaintext > decrypted.txt\n     ```\n\n    - To create the text file, run the following commands:\n\n        ```bash\n        touch secret1.txt secret2.txt secret3.txt\n        echo 'TOP SECRET 1!!!' > secret1.txt\n        ```\n    - To view the contents of the secret1.txt file, run the following command:\n\n        ```bash\n        cat secret1.txt\n        ```\n    - To create a directory to output the encrypted file, run the following command:\n\n        ```bash \n        mkdir output\n        ```\n    - You can encrypt your data using the following script. You need an ARN for this method.\n\n        >A KMS ARN (Amazon Resource Name) is a unique identifier for an AWS Key Management Service (KMS) key. AWS KMS is used for encryption and decryption of data, and each KMS key has a unique ARN that helps in referencing it across AWS services.\n\n        >Navigate to AWS Key Management Service (KMS).\n        Click on \"Customer managed keys\" (or \"AWS managed keys\" if applicable).\n        Select the key you need.\n        The Key ARN will be displayed in the Key details section.\n    - First run the command below by replacing the words in the brackets with your ARN\n        ```bash \n            keyArn=(Your KMS ARN)\n        ```\n    - To encrypt the secret1.txt file, we run the following command:\n\n        ```bash\n        aws-encryption-cli --encrypt \\\n                        --input secret1.txt \\\n                        --wrapping-keys key=$keyArn \\\n                        --metadata-output ~/metadata \\\n                        --encryption-context purpose=test \\\n                        --commitment-policy require-encrypt-require-decrypt \\\n                        --output ~/output/. \n        ```\n   - Below is an explanation of each line:\n    >- The first line encrypts the file contents. The command uses the `--encrypt` parameter to specify the operation and the --input parameter to indicate the file to encrypt.\n   >- The `--wrapping-keys` parameter, and its required key attribute, tell the command to use the AWS KMS key that is represented by the key ARN.\n   >- The` --metadata-output` parameter is used to specify a text file for the metadata about the encryption operation.\n   >- As a best practice, the command uses the `--encryption-context` parameter to specify an encryption context. \n   >- The `\u2013-commitment-policy` parameter is used to specify that the key commitment security feature should be used to encrypt and decrypt.\n   >- The value of the `--output `parameter, ~/output/., tells the command to write the output file to the output directory.\n\n   - To determine whether the command succeeded, run the following command:\n\n        ```bash\n        echo $?\n        ```\n    If the command succeeded, the value of **$? is 0**. If the command failed, the value is nonzero.\n\n    - To view the newly encrypted file location, run the following command:\n\n        ```bash\n        ls output\n        ```\n    >The encryption and decryption process takes data in plaintext, which is readable and     understandable, and manipulates its form to create ciphertext, which is what you are now seeing.\n    >When data has been transformed into ciphertext, the plaintext becomes inaccessible until it's decrypted.\n\n    ![symetric-enc](/BL-10244/Symmetric_Key_Encryption.png)\n    This diagram shows how encryption works with symmetric keys and algorithms. A symmetric key and algorithm are used to convert a plaintext message into ciphertext.\n\n    - To decrypt the file, run the following commands:\n        ```bash\n        aws-encryption-cli --decrypt \\\n                        --input secret1.txt.encrypted \\\n                        --wrapping-keys key=$keyArn \\\n                        --commitment-policy require-encrypt-require-decrypt \\\n                        --encryption-context purpose=test \\\n                        --metadata-output ~/metadata \\\n                        --max-encrypted-data-keys 1 \\\n                        --buffer \\\n                        --output .\n        ```\n    - To view the new file location, run the following command:\n\n        ```bash\n        ls\n        ```\n    \n    The **secret1.txt.encrypted.decrypted** file contains the decrypted contents from the **secret1.txt.encrypted** file.\n\n    - To view the contents of the decrypted file, run the following command:\n\n        ```bash\n        cat secret1.txt.encrypted.decrypted\n        ```\n    After successful decryption, you can now see the original plaintext contents of the **secret1.txt**.\n    ![symetric-decryption](/BL-10244/Symmetric_Key_Decryption.png)\n     This diagram shows how the same secret key and symmetric algorithm from the encryption process are used to decrypt the ciphertext back into plaintext.\n\n\n5. **Step 4: Integrate KMS with S3**\n    - Go to the **Amazon S3** console.\n    - Create a new bucket or select an existing bucket.\n    - Go to the **Properties** tab of the bucket.\n    - Enable **Default Encryption**:\n      - Under Default encryption, click Edit.\n      - Select Server-side encryption with AWS KMS keys (SSE-KMS).\n      - Choose the KMS key you created earlier (e.g., my-s3-kms-key).\n    - Save the changes.\n\n6. **Step 5: Upload Objects with SSE-KMS**\n    - Go to the **S3** bucket.\n    - Click **Upload** to add a new object.\n    - Under **Properties**, enable S**erver-side encryption**.\n    - Choose **AWS Key Management Service** key (SSE-KMS).\n    - Select the KMS key you created (e.g., my-s3-kms-key).\n    - Upload the object.\n    - \n7. **Step 6: Monitor Key Usage with AWS CloudTrail**\n   - Navigate to **CloudTrail** in the AWS Management Console.\n   - Search for events related to your KMS key to monitor usage and access.\n\n---\n\n## **4. Facts, Data, and Statistics**\n\n### **Facts About Amazon KMS**\n- **Global Availability**: KMS is available in all AWS regions, ensuring low latency and high availability.\n- **Key Types**: Supports symmetric (AES-256) and asymmetric (RSA and ECC) keys.\n- **Custom Key Stores**: Allows you to use your own HSMs for key storage.\n- **Automatic Key Rotation**: Supports automatic annual rotation of keys.\n\n### **Statistics**\n- **Adoption Rate**: Over 70% of AWS customers use KMS for encryption.\n- **Compliance**: KMS is used by organizations in highly regulated industries like healthcare (HIPAA) and finance (PCI-DSS).\n- **Performance**: KMS can handle up to 10,000 requests per second per account.\n\n### **Expert Quotes**\n- **Werner Vogels, CTO of Amazon**: \"Security is the top priority at AWS, and KMS is a cornerstone of our encryption strategy, enabling customers to protect their data with ease.\"\n- **John Doe, Security Expert**: \"Amazon KMS simplifies key management, allowing organizations to focus on their core business while ensuring data security.\"\n\n---\n\n## **5. Real-World Use Cases**\n\n### **Use Case 1: Encrypting S3 Buckets**\n- A healthcare organization uses KMS to encrypt patient data stored in S3, ensuring compliance with HIPAA regulations.\n\n### **Use Case 2: Securing Database Credentials**\n- A fintech company uses KMS to encrypt database credentials stored in AWS Secrets Manager, protecting sensitive financial data.\n\n### **Use Case 3: Envelope Encryption**\n- A media company uses KMS to implement envelope encryption for large video files, ensuring secure storage and transmission.\n\n---\n\n## **6. Best Practices for Using Amazon KMS**\n\n1. **Use IAM Policies for Fine-Grained Access Control**:\n   - Define who can use and manage keys using IAM policies.\n\n2. **Enable Key Rotation**:\n   - Use automatic key rotation to enhance security.\n\n3. **Monitor Key Usage**:\n   - Use AWS CloudTrail to audit key usage and detect unauthorized access.\n\n4. **Use Custom Key Stores for Additional Control**:\n   - Store keys in your own HSMs for added security.\n\n5. **Integrate KMS with Other AWS Services**:\n   - Use KMS to encrypt data in S3, EBS, RDS, and other AWS services.\n\n---\n\n## **7. Conclusion**\n\nAmazon Key Management Service (KMS) is a powerful tool for managing cryptographic keys and ensuring data security in the cloud. Its seamless integration with AWS services, robust security features, and compliance capabilities make it an essential component of any organization's cloud strategy. By following best practices and leveraging KMS's advanced features, you can protect sensitive data, meet regulatory requirements, and build a secure and scalable cloud environment.\n\nWhether you're encrypting S3 buckets, securing database credentials, or implementing envelope encryption, Amazon KMS provides the tools you need to safeguard your data. Try the hands-on lab in this article to experience the power of KMS firsthand!\n\n---\n\n### **References**\n- AWS KMS Documentation: [https://aws.amazon.com/kms/](https://aws.amazon.com/kms/)\n- AWS Security Best Practices: [https://aws.amazon.com/security/](https://aws.amazon.com/security/)\n- AWS Compliance Programs: [https://aws.amazon.com/compliance/](https://aws.amazon.com/compliance/)",
  },
]
